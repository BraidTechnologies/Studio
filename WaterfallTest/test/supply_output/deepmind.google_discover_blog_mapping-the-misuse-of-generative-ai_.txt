"\n\n\n\n\nMapping the misuse of generative AI - Google DeepMind\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n      Jump to Content\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle\n\n\n\n\nDeepMind\n\n\n\n\n\n\nSearch...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\nClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle\n\n\n\n\nDeepMind\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          About\n          \n\n\n\n\n\n\n\n\n\n\n          About\n          \n        \n\n — Our mission is to build AI responsibly to benefit humanity\n          \n\n\n\n\n\n\n\n          Responsibility & Safety\n          \n        \n\n — We want AI to benefit the world, so we must be thoughtful about how it’s built and used\n          \n\n\n\n\n\n\n\n          Education\n          \n        \n\n — Our vision is to help make the AI ecosystem more representative of society\n          \n\n\n\n\n\n\n\n          Careers\n          \n        \n\n — Many disciplines, one common goal\n          \n\n\n\n\n\n\n\n          Press\n          \n        \n\n — Toolkits and resources\n          \n\n\n\n\n\n\n\n\n\n          Research\n          \n        \n\n\n\n\n\n\n\n          Technologies\n          \n\n\n\n\n\n\n\n\n\n\n          View all technologies\n          \n        \n\n — Our next generation AI systems are solving some of the hardest scientific and engineering challenges of our time\n          \n\n\n\n\n\n\n\n          Gemini\n          \n\n\n\n\n — The most general and capable AI models we've ever built\n          \n\n\n\n\n\n\n\n          Gemini models\n          \n        \n\n — The Gemini family of models are the most general and capable AI models we've ever built.\n          \n\n\n\n\n\n\n\n          Ultra\n          \n        \n\n — Our largest model for highly complex tasks.\n          \n\n\n\n\n\n\n\n          Pro\n          \n        \n\n — Our best model for general performance across a wide range of tasks.\n          \n\n\n\n\n\n\n\n          Flash\n          \n        \n\n — Our lightweight model, optimized for speed and efficiency.\n          \n\n\n\n\n\n\n\n          Nano\n          \n        \n\n — Our most efficient model for on-device tasks.\n          \n\n\n\n\n\n\n\n\n\n          Project Astra\n          \n        \n\n — A universal AI agent that is helpful in everyday life\n          \n\n\n\n\n\n\n\n          Imagen\n          \n        \n\n — Our highest quality text-to-image model\n          \n\n\n\n\n\n\n\n          Veo\n          \n        \n\n — Our most capable generative video model\n          \n\n\n\n\n\n\n\n          SynthID\n          \n        \n\n — Identifying AI-generated content\n          \n\n\n\n\n\n\n\n\n\n          Impact\n          \n        \n\n\n\n\n\n\n\n          Discover\n          \n\n\n\n\n\n\n\n\n\n\n          Discover\n          \n        \n\n — Discover our latest breakthroughs. See how we’re shaping the future. Hear how AI can transform our world.\n          \n\n\n\n\n\n\n\n          Blog\n          \n        \n\n — Discover our latest AI breakthroughs, projects, and updates.\n          \n\n\n\n\n\n\n\n          Events\n          \n        \n\n — Meet our team and learn more about our research.\n          \n\n\n\n\n\n\n\n          The Podcast\n          \n        \n\n — Uncover the extraordinary ways AI is transforming our world on Google DeepMind: The Podcast.\n          \n\n\n\n\n\n\n\n          Visualising AI\n          \n        \n\n — Expanding our vision of what AI looks like.\n          \n\n\n\n\n\n\n\n\n\n\n\nSearch...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSearch\nClose\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          About\n          \n        \n\n — Our mission is to build AI responsibly to benefit humanity\n          \n\n\n\n\n\n\n\n          Responsibility & Safety\n          \n        \n\n — We want AI to benefit the world, so we must be thoughtful about how it’s built and used\n          \n\n\n\n\n\n\n\n          Education\n          \n        \n\n — Our vision is to help make the AI ecosystem more representative of society\n          \n\n\n\n\n\n\n\n          Careers\n          \n        \n\n — Many disciplines, one common goal\n          \n\n\n\n\n\n\n\n          Press\n          \n        \n\n — Toolkits and resources\n          \n\n\n\n\n\nLatest posts\n\n\n\n\n\n\n\nA new generation of African talent brings cutting-edge AI to scientific challenges\n5 August 2024\n\n\n\n\n\n\n\n\nMapping the misuse of generative AI\n2 August 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n          View all technologies\n          \n        \n\n — Our next generation AI systems are solving some of the hardest scientific and engineering challenges of our time\n          \n\n\n\n\n\n\n\n          Gemini\n          \n        \n\n — The most general and capable AI models we've ever built\n          \n\n\n\n\n\n\n\n          Project Astra\n          \n        \n\n — A universal AI agent that is helpful in everyday life\n          \n\n\n\n\n\n\n\n          Imagen\n          \n        \n\n — Our highest quality text-to-image model\n          \n\n\n\n\n\n\n\n          Veo\n          \n        \n\n — Our most capable generative video model\n          \n\n\n\n\n\n\n\n          SynthID\n          \n        \n\n — Identifying AI-generated content\n          \n\n\n\n\n\nLatest technology posts\n\n\n\n\n\n\n\nGemma Scope: helping the safety community shed light on the inner workings of language models\n31 July 2024\n\n\n\n\n\n\n\n\nGenerating audio for video\n17 June 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Discover\n          \n        \n\n — Discover our latest breakthroughs. See how we’re shaping the future. Hear how AI can transform our world.\n          \n\n\n\n\n\n\n\n          Blog\n          \n        \n\n — Discover our latest AI breakthroughs, projects, and updates.\n          \n\n\n\n\n\n\n\n          Events\n          \n        \n\n — Meet our team and learn more about our research.\n          \n\n\n\n\n\n\n\n          The Podcast\n          \n        \n\n — Uncover the extraordinary ways AI is transforming our world on Google DeepMind: The Podcast.\n          \n\n\n\n\n\n\n\n          Visualising AI\n          \n        \n\n — Expanding our vision of what AI looks like.\n          \n\n\n\n\n\nLatest posts\n\n\n\n\n\n\n\nA new generation of African talent brings cutting-edge AI to scientific challenges\n5 August 2024\n\n\n\n\n\n\n\n\nMapping the misuse of generative AI\n2 August 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nMapping the misuse of generative AI\n\nPublished\n2 August 2024\nAuthors\nNahema Marchal and Rachel Xu\n\n\n\n\n\n\n\n\n        Share\n      \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopy link\n\n\n                ×\n              \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew research analyzes the misuse of multimodal generative AI today, in order to help build safer and more responsible technologiesGenerative artificial intelligence (AI) models that can produce image, text, audio, video and more are enabling a new era of creativity and commercial opportunity. Yet, as these capabilities grow, so does the potential for their misuse, including manipulation, fraud, bullying or harassment.As part of our commitment to develop and use AI responsibly, we published a new paper, in partnership with Jigsaw and Google.org, analyzing how generative AI technologies are being misused today. Teams across Google are using this and other research to develop better safeguards for our generative AI technologies, amongst other safety initiatives.Together, we gathered and analyzed nearly 200 media reports capturing public incidents of misuse, published between January 2023 and March 2024. From these reports, we defined and categorized common tactics for misusing generative AI and found novel patterns in how these technologies are being exploited or compromised.By clarifying the current threats and tactics used across different types of generative AI outputs, our work can help shape AI governance and guide companies like Google and others building AI technologies in developing more comprehensive safety evaluations and mitigation strategies.\n\n\nHighlighting the main categories of misuseWhile generative AI tools represent a unique and compelling means to enhance creativity, the ability to produce bespoke, realistic content has the potential to be used in inappropriate ways by malicious actors.By analyzing media reports, we identified two main categories of generative AI misuse tactics: the exploitation of generative AI capabilities and the compromise of generative AI systems. Examples of the technologies being exploited included creating realistic depictions of human likenesses to impersonate public figures; while instances of the technologies being compromised included ‘jailbreaking’ to remove model safeguards and using adversarial inputs to cause malfunctions.\n\n\n\n\n\n\n\n\n\nRelative frequency generative AI misuse tactics in our dataset. Any given case of misuse reported in the media could involve one or more tactics.\n\n\n\nCases of exploitation — involving malicious actors exploiting easily accessible, consumer-level generative AI tools, often in ways that didn’t require advanced technical skills — were the most prevalent in our dataset. For example, we reviewed a high-profile case from February 2024 where an international company reportedly lost HK$200 million (approx. US $26M) after an employee was tricked into making a financial transfer during an online meeting. In this instance, every other “person” in the meeting, including the company’s chief financial officer, was in fact a convincing, computer-generated imposter.Some of the most prominent tactics we observed, such as impersonation, scams, and synthetic personas, pre-date the invention of generative AI and have long been used to influence the information ecosystem and manipulate others. But wider access to generative AI tools may alter the costs and incentives behind information manipulation, giving these age-old tactics new potency and potential, especially to those who previously lacked the technical sophistication to incorporate such tactics.\n\n\nIdentifying strategies and combinations of misuseFalsifying evidence and manipulating human likenesses underlie the most prevalent tactics in real-world cases of misuse. In the time period we analyzed, most cases of generative AI misuse were deployed in efforts to influence public opinion, enable scams or fraudulent activities, or to generate profit.By observing how bad actors combine their generative AI misuse tactics in pursuit of their various goals, we identified specific combinations of misuse and labeled these combinations as strategies.\n\n\n\n\n\n\n\n\n\nDiagram of how the goals of bad actors (left) map onto their strategies of misuse (right).\n\n\n\nEmerging forms of generative AI misuse, which aren’t overtly malicious, still raise ethical concerns. For example, new forms of political outreach are blurring the lines between authenticity and deception, such as government officials suddenly speaking a variety of voter-friendly languages without transparent disclosure that they’re using generative AI, and activists using the AI-generated voices of deceased victims to plead for gun reform.While the study provides novel insights on emerging forms of misuse, it’s worth noting that this dataset is a limited sample of media reports. Media reports may prioritize sensational incidents, which in turn may skew the dataset towards particular types of misuse. Detecting or reporting cases of misuse may also be more challenging for those involved because generative AI systems are so novel. The dataset also doesn’t make a direct comparison between misuse of generative AI systems and traditional content creation and manipulation tactics, such as image editing or setting up 'content farms' to create large amounts of text, video, gifs, images and more. So far, anecdotal evidence suggests that traditional content manipulation tactics remain more prevalent.\n\n\nStaying ahead of potential misusesOur paper highlights opportunities to design initiatives that protect the public, such as advancing broad generative AI literacy campaigns, developing better interventions to protect the public from bad actors, or forewarning people and equipping them to spot and refute the manipulative strategies used in generative AI misuse.This research helps our teams better safeguard our products by informing our development of safety initiatives. On YouTube, we now require creators to share when their work is meaningfully altered or synthetically generated, and seems realistic. Similarly, we updated our election advertising policies to require advertisers to disclose when their election ads include material that has been digitally altered or generated.As we continue to expand our understanding of malicious uses of generative AI and make further technical advancements, we know it’s more important than ever to make sure our work isn’t happening in a silo. We recently joined the Content for Coalition Provenance and Authenticity (C2PA) as a steering committee member to help develop the technical standard and drive adoption of Content Credentials, which are tamper-resistant metadata that shows how content was made and edited over time.In parallel, we’re also conducting research that advances existing red-teaming efforts, including improving best practices for testing the safety of large language models (LLMs), and developing pioneering tools to make AI-generated content easier to identify, such as SynthID, which is being integrated into a growing range of products.In recent years, Jigsaw has conducted research with misinformation creators to understand the tools and tactics they use, developed prebunking videos to forewarn people of attempts to manipulate them, and shown that prebunking campaigns can improve misinformation resilience at scale. This work forms part of Jigsaw’s broader portfolio of information interventions to help people protect themselves online.By proactively addressing potential misuses, we can foster responsible and ethical use of generative AI, while minimizing its risks. We hope these insights on the most common misuse tactics and strategies will help researchers, policymakers, industry trust and safety teams build safer, more responsible technologies and develop better measures to combat misuse.\n\n\n\n\n      Read our paper\n      \n\n\n\n\n      Learn more about Jigsaw\n      \n\n\n\n\n\n\n\n\nAcknowledgementsThis research was a collective effort by Nahema Marchal, Rachel Xu, Rasmi Elasmar, Iason Gabriel, Beth Goldberg, and William Isaac, with feedback and advisory contributions from Mikel Rodriguez, Vijay Bolina, Alexios Mantzarlis, Seliem El-Sayed, Mevan Babakar, Matt Botvinick, Canfer Akbulut, Harry Law, Sébastien Krier, Ziad Reslan, Boxi Wu, Frankie Garcia, and Jennie Brennan.\n\n\n\n\n\n\nRelated posts\n\n\n\n      View all posts\n      \n    \n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nThe ethics of advanced AI assistants\nExploring the promise and risks of a future with more capable AI\n\n\n19 April 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nEvaluating social and ethical risks from generative AI\nIntroducing a context-based framework for comprehensively evaluating the social and ethical risks of AI systems\n\n\n19 October 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nAn early warning system for novel AI risks\nNew research proposes a framework for evaluating general-purpose models against novel threats\n\n\n25 May 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nExploring institutions for global AI governance\nNew white paper investigates models and functions of international institutions that could help manage opportunities and mitigate risks of advanced AI.\n\n\n11 July 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nHow can we build human values into AI?\nDrawing from philosophy to identify fair principles for ethical AI...\n\n\n24 April 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nBest practices for data enrichment\nBuilding a responsible approach to data collection with the Partnership on AI...\n\n\n16 November 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResponsibility & Safety\nLanguage modelling at scale: Gopher, ethical considerations, and retrieval\nLanguage, and its role in demonstrating and facilitating comprehension - or intelligence - is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts,...\n\n\n8 December 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFooter links\n\n\n\nFollow us\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout\n\n\n\n\n\n\n\n\nAbout Google DeepMind\n\n\nResponsibility & Safety\n\n\nEducation\n\n\nCareers\n\n\nPress\n\n\n\n\n\n\n\nLearn more\n\n\n\n\n\n\n\n\nResearch\n\n\nTechnologies\n\n\nImpact\n\n\nDiscover\n\n\n\n\n\n\n                    Sign up for updates on our latest innovations\n                  \n\n\n\nEmail address\n\n\n\n\n\n\n\n\n\n\nPlease enter a valid email (e.g., \"name@example.com\")\n\n\n\n\nI accept Google's Terms and Conditions and acknowledge that my information will be used in accordance with Google's Privacy Policy.Sign up\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbout Google\n\n\nGoogle products\n\n\nPrivacy\n\n\nTerms\n\n\nCookies management controls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"