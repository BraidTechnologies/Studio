"Managing the risks around generative AI | McKinsey\n\nSkip to main contentPlease use UP and DOWN arrow keys to review autocomplete results. Press enter to select and open the results on a new page.SearchSign In|SubscribeManaging the risks around generative AISharePrintDownloadSaveManaging the risks around generative AIJune 12, 2024 | PodcastSharePrintDownloadSaveThe transformational technology will require every member of an organization to be a risk professional.DOWNLOADSArticle (6 pages)\nGenerative AI (gen AI) is an advancement that has seized the attention of governments, the public, and business leaders. But it presents unique challenges to the professionals tasked with managing gen AI’s existing and potential threats. In this episode of the Inside the Strategy Room podcast, Ida Kristensen, coleader  of McKinsey’s Risk & Resilience Practice, and Oliver Bevan, a leader of enterprise risk management, speak with Sean Brown, global director of marketing and communications  for the Strategy & Corporate Finance Practice, about the ways business leaders should approach the rapidly evolving impact of the technology.\nThis is an edited transcript of their conversation. For more discussions on the strategy issues that matter, follow the series on your preferred podcast platform.\n\nSean Brown: Given how recent this technology is, it’s amazing how much of an evolution we’ve seen and how much development and deployment. But as with all new technologies, there are risks—and maybe more so with this one than with some others. Ida, are you finding your clients widely embracing gen AI?\nMost Popular InsightsMcKinsey technology trends outlook 2024What to read next: McKinsey’s 2024 annual book recommendationsThe loneliest job? How top CEOs manage dilemmas and vulnerabilityMost Popular InsightsMcKinsey technology trends outlook 2024What to read next: McKinsey’s 2024 annual book recommendationsThe loneliest job? How top CEOs manage dilemmas and vulnerabilityThe hard stuff: Navigating the physical realities of the energy transitionThe state of AI in early 2024: Gen AI adoption spikes and starts to generate value\nIda Kristensen: We see the early adopters that are incredibly excited. We also see many that are quite skeptical and say, “Ah, you know, maybe we’ll wait this one out.” At McKinsey, we are very much in the camp of saying, “Gen AI is here to stay. It offers incredible strategic opportunities across industries and across almost every aspect of what businesses do.”\nThere is a path to extracting amazing benefits. We also believe that the idea of waiting this one out is not feasible. This is becoming a strategic imperative. Given we are talking about this from a risk angle, another way to say the same thing is that there is a substantial strategic risk associated with not diving in.\nThat said, there are some real risks associated with deploying gen AI. For an organization to be successful, it requires a defensive, as well as offensive, strategy.\nSean Brown: So, Oliver, given all these risks, what are the regulatory dynamics that companies need to be mindful of?\nOliver Bevan: It’s important to understand how different jurisdictions are taking different approaches to thinking about the risks of gen AI and how they want to govern and approach the overall management of them.\nSubscribe to the Inside the Strategy Room podcastApple PodcastsSpotify\nThis is importantly different from what we saw in the early days of data privacy. With data privacy, there was a sense that GDPR1The General Data Protection Regulation is a regulation on information privacy in the European Union and the European Economic Area. [General Data Protection Regulation] really led the way. A lot of our clients had adapted their initial data privacy frameworks to GDPR and then thought about adapting them globally using GDPR as a foundation.\nWe saw a lot of legislation that followed in the steps of GDPR. Some of what was happening in California drew directly on GDPR as well. Our sense is that’s going to be much trickier to do with gen AI.\nIt’s also clear to a lot of the public sector how much value is at stake and how much AI can directly affect citizens. Obviously, the risks are across data privacy, cybersecurity, and consent around deepfakes, which can have a significant impact on elections and other public-facing events.\nThat’s why many public actors have taken much more of a front foot approach to thinking about gen AI and, in turn, the responses that organizations need to have. It’s especially true if you’re operating across multiple jurisdictions and thinking about regulatory change. Adapting and embedding these responses into your approach is going to be incredibly important.\nShare SidebarAbout QuantumBlack, AI by McKinseyQuantumBlack, McKinsey’s AI arm, helps companies transform using the power of technology, technical expertise, and industry experts. With thousands of practitioners at QuantumBlack (data engineers, data scientists, product managers, designers, and software engineers) and McKinsey (industry and domain experts), we are working to solve the world’s most important AI challenges. QuantumBlack Labs is our center of technology development and client innovation, which has been driving cutting-edge advancements and developments in AI through locations across the globe.\nSean Brown: How do the risks and the regulatory landscape differ between gen AI and some of the other advances in AI, including machine learning?\nOliver Bevan: My perspective is that analytical AI, the AI development behind machine learning prior to the adoption of gen AI, is basically anchored in two primary places.\nOne is data privacy, so that’s considerations around, “How are these models using data? How are you combining data to do synthetic analysis or increase the potential for outputs?”\nThe second is really on questions around fairness in the model structure. We’re all aware unfortunately that these models tend to have very different outcomes, depending on whether you are looking at different skin tones or different genders. Analytical AI triggered many concerns, especially in the US, around fair housing legislation and optimizing credit decisions in financial services.\nNow, there is an increasing awareness of the challenges that come from explainability, for example.\nObviously, there is potential for deepfakes and malicious use of gen AI as the technology can create compelling and realistic facsimiles of either individual identities or corporate identities, which creates massive reputational risks and challenges for governments as well.\nIda Kristensen: It’s a fast evolution, but it is still an evolution of the risks that have been around for a long time, with a couple of twists.\nThere is the fairness aspect: in most industries, part of the solution has been creating real transparency about how a model works. If you have a more traditional analytical model, whether you regulate it or not, you can explain exactly what happened. That’s part of the reason you should get comfortable with the model.\nIt is a very different game with gen AI, because the explainability of these models is just, let’s be honest, not quite there. While the risks might be the same, it’ll be very interesting to see how different companies and regulators are going to get comfortable with something that can’t be opened and dissected—where we’ve got to rely on other indicators to get comfortable with fairness.\nSean Brown: Are you seeing any common areas that governments and regulators are focusing on in terms of risk? Or is it primarily explainability?\nOliver Bevan: The most important one is a deeper understanding of how these models work and how these actors are going to have confidence that the models are producing outcomes that can be explained in some fashion.\nExplainability is a foundational challenge with gen AI models, as well as sourcing. There is a lot of discussion now about watermarks on gen AI. Can you tell when something’s been generated by AI versus generated by our traditional creative process?\nThat flows into intellectual property challenges. I suspect, given the raft of elections that are coming up, there’s going to be a lot of focus on those dynamics from the public sector. It’s also incredibly important to think about public trust in these systems and public willingness to use and engage them.\nIda Kristensen: No one will be shocked by our saying that it’s still early days for regulation. We expect much more to come, including sector-specific regulation for regulated industries. There’s no doubt that financial services will face more specific regulation on top of this. So thinking about themes is the right way to do it. Trying to optimize what’s already out there will be a very short-lived strategy.\nSean Brown: For those companies that have been at this for a while, what principles are they following to make sure they use gen AI safely?\nIda Kristensen: For one, don’t let the machines run wild on their own. There’s always a human aspect involved in that. You use the outcome of the models as an input to human decisioning, not as the final decision.\nThe good news is, gen AI makes it a lot easier to do a lot of rapid tests for fairness. Gen AI can be a real source for good—also in terms of putting in place strong risk management capabilities. Maybe the biggest step change from the responsible AI programs that most organizations have already been working on is the transparency and explainability we talked about already, and then monitoring and evaluation.\nIt’s a different beast to keep an eye on gen AI’s evolution  over time. Therefore, we see clients we work with investing in monitoring gen AI and ask, “Okay, what are the extra bells and whistles and checks we can put in place to get comfortable with what comes out?”\nSean Brown: Any other risks that are out there? And how should companies think about the full gamut of risks that can emerge with gen AI?\nIda Kristensen: Data privacy aspects and quality. One of the things we did at McKinsey, for instance, was we created a network [database] of all our proprietary knowledge [and more] and used that as the training data for some of our applications. That means we control the quality of the data that comes in, and that gives us a great deal of comfort.\nMalicious use has been more headline-grabbing because of deepfakes and scams. Consider a malicious actor, for instance, who may impersonate you and write an email asking your   uncle to wire them $1,000 tomorrow. They can write those emails; they can translate them into every language. We’ve all grown aware of our personal risk and become used to spam emails. There used to be telltale signs, right? Bad grammar, bad language, and things that just didn’t quite make sense. But now it is so much easier to create high-quality spam emails using gen AI.\nFinally, there is strategic risk and third-party risk. Strategic risk is about where you play and how you competitively place yourself, but it also has all the aspects of the broader societal impact. We all know gen AI uses quite a lot of computational power. How does that mesh with your ESG [environmental, social, and governance] commitments? There are broader employment effects as well.\nWhat does it do to the workforce? How do you think about your commitment to your employees and the shift that gen AI will bring? As we mentioned, this is not a technology that’s going to take away net jobs, but it’s a technology that’s going to change jobs very dramatically.\nThose are real questions to grapple with.\nSean Brown: Now, if I understand correctly, what you put into your queries can actually feed the model. Does that mean you need to monitor what your employees are asking gen AI?\nIda Kristensen: Yes, it’s potentially scary, right? Any prompt that is answered becomes part of that body of data as well. So most companies should be worried about what their employees are doing.\nYou’ve got to educate your employees. You can’t just rely on rules. You’ve got to make sure your employees understand the consequences of any prompt they put into a system.\nSean Brown: I’d love to talk a bit more about what the risk management approach model should look like to cover all these novel risks. How are you advising clients on this?\nOliver Bevan: We basically have four categories. One is principles and guardrails. The second is frameworks. The third is deployment and governance. And the fourth is risk mitigation and monitoring.\nOn principles and guardrails, it’s incredibly important to have an honest conversation as an executive team about how and where you want to use gen AI, while you’re thinking about segmenting your use cases.\nSome examples that come up typically are considerations of the degree to which you want to use gen AI to personalize your marketing—for example, the extent to which you want gen AI to be used in performance evaluations or in directly engaging with your employees.\nOn frameworks, Ida’s already talked about the taxonomy, so I won’t go into that, except to note that there are different flavors of the risks from gen AI and different ways of thinking about them. Having something that works for your organization is incredibly valuable, because it will help you communicate to your employees how they should be thinking about this.\nOn risk identification, you must have a handle on what types of risks you’re going to face. What a lot of our clients are doing is starting with lower-risk, easier-implementation use cases. That allows them the time to experiment with the governance standards they put up. It allows them time to run the use cases through something like a risk assessment to understand the typical types of risks that they’re exposed to from the use cases.\nSean Brown: I’d love to understand a bit better how companies should approach mitigation of external risks. Let’s take security threats: what kind of measures should they put in place?\nIda Kristensen: It is a mix of tried and tested risk management techniques building on what already exists, potentially turbocharging a few of them and then adding a few more tools to the tricks.\nEmployees truly need to understand risk 101 of gen AI to be alert and to identify and spot when things are happening. Get people under the tent.\nSecurity is an area where you must fight gen AI with gen AI. Most organizations now really work through it and say, “How do we use gen AI tools to turbocharge our cyber defense, turbocharge the way we do pin testing, the way we think about the different layers of our security, and make sure that our time to detection is way faster?” The time to detection will need to be faster, and the time from detection to shutting things down will need to be faster. That will be super critical.\nSean Brown: Ida, you mentioned you can use AI to fight AI. How could you use gen AI to find deepfakes, for example? I can envision an email that looks and reads like it’s coming from the CEO, directing you to take certain steps. This starts to seem like something you might see in a movie.\nIda Kristensen: Some of it will be process changes. As I mentioned, you might say, “Hey, we were used to, if an ask comes through an email, you just go do it.” No longer. If there’s a voicemail left for you, company policy is you will never act on that voicemail unless you call someone back and you get verification.\nThere are controls and oversight you could put in place, but, and this is a big theme of what we’re talking about today, it’ll never be enough. Let’s be honest. Controls always have to go hand in hand with this awareness of our employees. You’ve got to make sure that you have people who say, “That’s a little weird. That guy never leaves me a voicemail.”\nRisk management is everyone’s job. It needs to get embedded in the fabric and the culture of how we work together.\nSean Brown: What are the typical practices you see clients adopt as they scale internal use of AI?\nOliver Bevan: Overreliance on a small group of experts. Obviously, at the start when you’re building use cases, you will have potentially limited capabilities internally. You’ll also have a group of acolytes or very excited people who want to spend a lot of their time working on gen AI. That small group of experts is going to rapidly get overwhelmed as you scale gen AI. It’s going to create a lot of friction, a lot of frustration, and it’s going to slow everything down.\nRelying solely on vendors is also not something we’d recommend, frankly even at the early stages. There’s wide variation in terms of what the large language model providers and the other third-party ecosystems are doing. You need to take responsibility for diligence on gen AI, and you need to think about what you can do internally beyond just what they offer as out-of-the-box security solutions. Similarly, just having technical mitigation strategies is typically not going to be sufficient.\nWe’re still learning a lot about how the different controls and mitigations perform. And so you need some overlay of human factors, including things like having a human in the loop. Integrate risk and development groups as soon as you can. Given the evolution and dynamics of gen AI, be aware that these will evolve and change over time and that you need to have ways to track that evolution for successful and sustainable scaling.\nSean Brown: Any final thoughts before we close?\nIda Kristensen: Yes. If in our discussion of risk, it came across as doomsday, we apologize. In my mind, it’s the classic idea of building better brakes to go faster. That’s what it is. In addition, let’s call me a seasoned risk practitioner.\nI’m incredibly excited as well about this idea of shifting left. It is something we have talked about for many years—and it’s been hard to implement. We now have enough value at stake that we will begin to see a more seamless collaboration, in an agile way, between risk and the development of gen AI. So, yes, we’ll unleash all this potential from the technology but hopefully also get it right in terms of working together and embedding risk much earlier, which would make an old practitioner like me very happy.About the author(s)Ida Kristensen is a senior partner in McKinsey’s New York office, and Oliver Bevan is a partner in the Chicago office. Sean Brown is global director of communications for the Strategy & Corporate Finance Practice and is based in the Boston office.\n\nThis article was edited by David Weidner, a senior editor in the Bay Area office.Talk to usExplore a career with usSearch OpeningsRelated ArticlesArticle - McKinsey QuarterlyImplementing generative AI with speed and safetyWebcastThe economic potential of generative AI: The next productivity frontierArticleGen AI: Opportunities in M&ASign up for emails on new Strategy articlesNever miss an insight. We'll email you when new articles are published on this topic.SubscribeSign up for emails on new Strategy articlesWe use cookies to give you the best possible experience with mckinsey.com. Some are essential for this site to function; others help us understand how you use the site, so we can improve it. We may also use cookies for targeting purposes. Click “Accept all cookies” to proceed as specified, “Decline optional cookies” to accept only essential cookies, or click “Manage my preferences” to choose what cookie types you will accept.Cookie NoticeAccept All CookiesDecline optional cookies Manage my preferencesPrivacy Preference CenterMcKinsey and our trusted partners use cookies and similar technologies to access and use your data for the purposes listed below. Please provide your consent for cookie usage on this website. Enable one or more of the cookie types listed below, and then save your preferences.\n            Cookie NoticeAccept all cookies Manage Consent PreferencesPerformance Cookies  Performance Cookies These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site and app. They help us to know which pages are the most and least popular and see how visitors move around the site and app. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site or app, and will not be able to monitor its performance.Cookie details‎Functional Cookies  Functional Cookies These cookies enable the website and app to provide enhanced functionality and personalization. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.Cookie details‎Targeting Cookies  Targeting Cookies These cookies may be set through our site or app by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.Cookie details‎Strictly Necessary CookiesAlways ActiveThese cookies are necessary for the website and app to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site and app will not then work. These cookies do not store any personally identifiable information.Cookie details‎Back ButtonPerformance Cookies  Search IconFilter IconClear checkbox label labelApply CancelConsent Leg.Interest checkbox label label checkbox label label checkbox label labelView CookiesNamecookie name Save my preferences"