{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_XLSX_path = 'D:\\\\Braid Technologies\\\\BraidTechnologiesRepo\\\\WorkedExamples\\\\BoxerTest\\\\test output\\\\Test Results 2024052802_Baseline.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "output_XLSX_df = pd.read_excel(output_XLSX_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1.question</th>\n",
       "      <th>Column1.hit</th>\n",
       "      <th>Column1.summary</th>\n",
       "      <th>Column1.hitRelevance</th>\n",
       "      <th>Column1.followUp</th>\n",
       "      <th>Column1.followUpOnTopic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are LLMs different from traditional AI mod...</td>\n",
       "      <td>True</td>\n",
       "      <td>LLMs, or Large Language Models, are powerful t...</td>\n",
       "      <td>0.878444</td>\n",
       "      <td>That's correct! LLMs are indeed powerful, but ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a Large Language Model (LLM)?</td>\n",
       "      <td>True</td>\n",
       "      <td>LLMs, or Large Language Models, are powerful t...</td>\n",
       "      <td>0.877488</td>\n",
       "      <td>That's correct! LLMs are indeed powerful, but ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is natural language processing (NLP)?</td>\n",
       "      <td>True</td>\n",
       "      <td>In this lesson, we will learn how to analyze n...</td>\n",
       "      <td>0.874668</td>\n",
       "      <td>That sounds like an interesting project! Natur...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are prompt engineering techniques and how...</td>\n",
       "      <td>True</td>\n",
       "      <td>Prompt engineering is a discipline focused on ...</td>\n",
       "      <td>0.874642</td>\n",
       "      <td>That's correct! Prompt engineering is all abou...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between supervised, uns...</td>\n",
       "      <td>True</td>\n",
       "      <td>Reinforcement learning is an important aspect ...</td>\n",
       "      <td>0.867244</td>\n",
       "      <td>Yes, that's correct! Reinforcement learning is...</td>\n",
       "      <td>Yes, that's correct! The combination of reinfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Column1.question  Column1.hit  \\\n",
       "0  How are LLMs different from traditional AI mod...         True   \n",
       "1              What is a Large Language Model (LLM)?         True   \n",
       "2         What is natural language processing (NLP)?         True   \n",
       "3  What are prompt engineering techniques and how...         True   \n",
       "4  What is the difference between supervised, uns...         True   \n",
       "\n",
       "                                     Column1.summary  Column1.hitRelevance  \\\n",
       "0  LLMs, or Large Language Models, are powerful t...              0.878444   \n",
       "1  LLMs, or Large Language Models, are powerful t...              0.877488   \n",
       "2  In this lesson, we will learn how to analyze n...              0.874668   \n",
       "3  Prompt engineering is a discipline focused on ...              0.874642   \n",
       "4  Reinforcement learning is an important aspect ...              0.867244   \n",
       "\n",
       "                                    Column1.followUp  \\\n",
       "0  That's correct! LLMs are indeed powerful, but ...   \n",
       "1  That's correct! LLMs are indeed powerful, but ...   \n",
       "2  That sounds like an interesting project! Natur...   \n",
       "3  That's correct! Prompt engineering is all abou...   \n",
       "4  Yes, that's correct! Reinforcement learning is...   \n",
       "\n",
       "                             Column1.followUpOnTopic  \n",
       "0                                                 No  \n",
       "1                                                 No  \n",
       "2                                                Yes  \n",
       "3                                                Yes  \n",
       "4  Yes, that's correct! The combination of reinfo...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_XLSX_df.head()  # Print the first 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99, 6)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the DataFrame\n",
    "print(output_XLSX_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Column1.question         99 non-null     object \n",
      " 1   Column1.hit              99 non-null     bool   \n",
      " 2   Column1.summary          99 non-null     object \n",
      " 3   Column1.hitRelevance     99 non-null     float64\n",
      " 4   Column1.followUp         99 non-null     object \n",
      " 5   Column1.followUpOnTopic  99 non-null     object \n",
      "dtypes: bool(1), float64(1), object(4)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "output_XLSX_df.info()   # Print information about the DataFrame (number of non-null values, data types, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1.hitRelevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.819454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.776348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.802420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.815929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.834772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.878444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Column1.hitRelevance\n",
       "count             99.000000\n",
       "mean               0.819454\n",
       "std                0.023639\n",
       "min                0.776348\n",
       "25%                0.802420\n",
       "50%                0.815929\n",
       "75%                0.834772\n",
       "max                0.878444"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_XLSX_df.describe() # Print summary statistics for each column in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Column1.question', 'Column1.hit', 'Column1.summary',\n",
       "       'Column1.hitRelevance', 'Column1.followUp', 'Column1.followUpOnTopic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_XLSX_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_baseline = output_XLSX_df['Column1.question'].tolist()\n",
    "followUps_baseline = output_XLSX_df['Column1.followUp'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How are LLMs different from traditional AI models?',\n",
       " 'What is a Large Language Model (LLM)?',\n",
       " 'What is natural language processing (NLP)?',\n",
       " 'What are prompt engineering techniques and how do they work?',\n",
       " 'What is the difference between supervised, unsupervised, and reinforcement learning?',\n",
       " 'How can LLMs be used for chatbots?',\n",
       " 'What are the considerations for using LLMs in voice assistants?',\n",
       " \"What are the pricing models for popular LLM services like OpenAI's GPT?\",\n",
       " \"How does OpenAI's GPT-4 compare to other models like Google's BERT?\",\n",
       " \"How do I use Hugging Face's Transformers library?\",\n",
       " 'How does NLP relate to LLMs?',\n",
       " 'What are the methods for implementing sentiment analysis using LLMs?',\n",
       " 'What are the computational requirements for training an LLM?',\n",
       " 'How do I handle bias in training data?',\n",
       " 'How can LLMs assist in language translation applications?',\n",
       " 'What are the techniques for chaining LLM responses for complex tasks?',\n",
       " 'What is the role of LLMs in automated code generation?',\n",
       " 'What is the role of the Hugging Face Model Hub in working with LLMs?',\n",
       " 'How can LLMs be used for content generation, such as blog posts or articles?',\n",
       " 'How can LLMs be used for data extraction from unstructured text?',\n",
       " 'How do I fine-tune a pre-trained LLM on my own dataset?',\n",
       " 'How do I use TensorFlow or PyTorch with LLMs?',\n",
       " 'What is transfer learning and how does it apply to LLMs?',\n",
       " 'How do emerging models like GPT-4.5 or GPT-5 compare to GPT-4?',\n",
       " 'How much data do I need to train or fine-tune an LLM effectively?',\n",
       " 'How do I implement contextual understanding in my LLM-based application?',\n",
       " 'What are some common use cases for LLMs in applications?',\n",
       " 'How do LLMs process and generate text?',\n",
       " 'What are the steps to create a question-answering system with an LLM?',\n",
       " 'What are the latest advancements in LLM technology?',\n",
       " 'What are the most popular LLMs available today (eg GPT-4, BERT, T5)?',\n",
       " 'How are LLMs trained?',\n",
       " 'What future applications and improvements are expected for LLMs?',\n",
       " 'What are the uses of LLMs in customer service?',\n",
       " 'What are the common issues faced when integrating LLMs?',\n",
       " 'What datasets are commonly used for training LLMs?',\n",
       " 'What are the best practices for scaling LLM infrastructure?',\n",
       " 'How do I gather and use user feedback to improve my LLM-based application?',\n",
       " 'What are the GDPR implications of using LLMs?',\n",
       " 'How do LLMs work?',\n",
       " 'What are the privacy concerns when using LLMs?',\n",
       " 'What are the risks of using LLMs and how can I mitigate them?',\n",
       " 'What are the key components of an LLM?',\n",
       " 'How do I scale an LLM-based application to handle increased traffic?',\n",
       " 'What is the process for deploying an LLM-based application?',\n",
       " 'What are some common performance bottlenecks when using LLMs?',\n",
       " 'How have other developers solved common problems with LLMs?',\n",
       " 'How do I monitor and maintain an LLM-based application in production?',\n",
       " 'How can I use LLMs for specific domain applications, like medical or legal?',\n",
       " 'What metrics should I use to evaluate the performance of my LLM?',\n",
       " 'How do I handle API rate limits when using a hosted LLM service?',\n",
       " 'What are the best courses or tutorials for learning to use LLMs?',\n",
       " 'How do I evaluate the performance of different LLMs?',\n",
       " 'How can LLMs benefit the education sector?',\n",
       " 'What cloud services are recommended for hosting LLM-based applications?',\n",
       " 'How can I use an LLM to summarize text?',\n",
       " 'How can I minimize the cost of API usage for LLMs?',\n",
       " 'What techniques can I use to improve the accuracy of my LLM?',\n",
       " 'What are the methods to evaluate the relevance of LLM responses?',\n",
       " 'What are the legal implications of using LLMs in different industries?',\n",
       " 'What are the ethical considerations when using LLMs in applications?',\n",
       " 'How can I optimize the performance of an LLM in production?',\n",
       " 'How can I personalize LLM interactions for individual users?',\n",
       " 'How is the field of LLMs expected to evolve over the next 5 years?',\n",
       " 'How often should I update or retrain my LLM?',\n",
       " 'How do I measure the quality of the generated text?',\n",
       " 'Can I use pre-trained models or do I need to train my own from scratch?',\n",
       " 'How can I use load balancing with LLMs?',\n",
       " 'How are LLMs used in the healthcare industry?',\n",
       " 'What security measures should I implement when using LLMs?',\n",
       " 'What are the best tools for annotating and preparing training data?',\n",
       " 'How can I customize the behavior of an LLM to better fit my application?',\n",
       " 'How can I contribute to the development of open-source LLM projects?',\n",
       " 'What online communities and forums are best for learning about LLMs?',\n",
       " 'What are the copyright considerations for content generated by LLMs?',\n",
       " 'How do I manage version control for my LLM models?',\n",
       " 'What are some successful case studies of LLM integration?',\n",
       " 'What are the applications of LLMs in finance?',\n",
       " 'What strategies can I use to make LLM responses more engaging?',\n",
       " 'What libraries or frameworks are available for working with LLMs in Python?',\n",
       " 'How can I use Docker to deploy LLM-based applications?',\n",
       " 'What factors should I consider when choosing an LLM for my application?',\n",
       " 'How do I estimate the cost of using an LLM in my application?',\n",
       " 'What are the signs that my LLM needs retraining?',\n",
       " 'What are the cost considerations when choosing between different LLM providers?',\n",
       " 'How can I ensure that my LLM is not producing biased or harmful content?',\n",
       " 'How do I integrate an LLM into my Python application?',\n",
       " 'How can I ensure my use of LLMs complies with industry regulations?',\n",
       " 'How do I manage user data responsibly in an LLM-based application?',\n",
       " 'How do LLMs apply to the entertainment and media industry?',\n",
       " 'How do I protect my LLM from adversarial attacks?',\n",
       " 'How do I debug issues with LLM-generated content?',\n",
       " 'How can I optimize the response time of an LLM in my application?',\n",
       " 'How can I ensure secure communication between my application and the LLM API?',\n",
       " 'How can I reduce the latency of LLM responses?',\n",
       " 'How do I determine the size of the model I need?What are the trade-offs between smaller and larger models?',\n",
       " 'What caching strategies can I use to improve LLM response times?',\n",
       " 'How can I track and fix inaccuracies in LLM responses?',\n",
       " 'What are the best practices for managing API keys and authentication?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'That sounds like an interesting project! Natural Language Processing (NLP) is a field of AI that focuses on understanding and analyzing human language. The Hugging Face ecosystem and Transformers library provide tools and models to help with NLP tasks like classifying the similarity of phrases. It can be applied in various fields like marketing, logistics, and medicine.',\n",
       " \"That's correct! Prompt engineering is all about finding the best way to use large language models (LLMs) by designing effective prompts. It helps improve the performance of LLMs in tasks like question answering and arithmetic reasoning, and can also make them safer and more knowledgeable by incorporating domain expertise and external tools.\",\n",
       " \"Yes, that's correct! Reinforcement learning is limited in its ability to learn complex dependencies, but unsupervised learning can provide the necessary information. By combining the two, we can create more effective models for training machines to maximize value functions and make predictions about the world. It's a powerful approach in AI.\",\n",
       " \"That's correct! Designing chatbots involves techniques like retrieval-augmented generation and routing between data sources. Optimizations can make them more powerful but add complexity. The guide provides an overview of implementing features and tailoring chatbots to specific use-cases. Do you have any specific questions about chatbot design?\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'That sounds like a comprehensive blog post that covers various large language models like BERT, T5, and GPT, and their applications in natural language processing tasks. It also discusses the training methods and datasets used for these models. The post mentions instruction tuning and retrieval-enhancing models as well. It seems to provide insights into current trends and future directions of large language models.',\n",
       " 'That sounds like a great course for learning NLP using the Hugging Face ecosystem! It covers topics like Transformer models, fine-tuning pretrained models, using the Datasets and Tokenizers libraries, and applying Transformer models to speech processing and computer vision. Prior knowledge of Python and deep learning is recommended.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's a great summary! Understanding bias and variance helps us build better machine learning models. Regularization helps reduce overfitting (high variance), train-dev-test splits help evaluate model performance, and model selection and cross-validation help choose the best model with the right balance of bias and variance.\",\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " \"Yes, you're correct! Chains involve sequencing language model calls to build context, while plugins allow the model to use external tools. Chains are reliable because they control the flow, while plugins offer flexibility. Both approaches enhance language models by incorporating external data and tools to improve their capabilities.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's great! Large Language Models (LLMs) can indeed be used for information extraction. The three approaches you mentioned - tool/function calling mode, JSON mode, and prompting based - are different ways to extract structured output. The quickstart example likely demonstrates how to call a tool or function to extract information. Additional resources can provide guidelines, reference examples, and other extraction tools.\",\n",
       " \"That's correct! Fine-tuning allows us to take a pre-trained language model and adapt it to perform better on specific tasks. It's a powerful technique, but it does require expertise and careful consideration to ensure it's the right approach for the use case.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'That sounds like a comprehensive course on large language models (LLMs) and their applications in machine learning. It covers core machine learning concepts, the Transformer architecture, and popular LLMs like GPT and BERT. It also includes details on running a Transformer. The video provides an overview of these topics.',\n",
       " 'That sounds like a comprehensive blog post that covers various large language models like BERT, T5, and GPT, and their applications in natural language processing tasks. It also discusses the training methods and datasets used for these models. The post mentions instruction tuning and retrieval-enhancing models as well. It seems to provide insights into current trends and future directions of large language models.',\n",
       " \"That's correct! Fine-tuning allows us to take a pre-trained language model and adapt it to a specific task or domain. It can be helpful when other techniques like prompt engineering or retrieval-augmented generation have limitations. However, it's important to have expertise and carefully consider the use case and alternatives before fine-tuning.\",\n",
       " 'That sounds like a comprehensive video that explores different ways to enhance language models with external context. It discusses techniques like retrieval augmentation, chaining, and tool use to improve the performance of language models. It also covers information retrieval techniques and how to work with the limited context window of language models.',\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'That sounds like a comprehensive course on large language models (LLMs) and their applications in machine learning. It covers core machine learning concepts, the Transformer architecture, and popular LLMs like GPT and BERT. It also includes details on running a Transformer. The video provides an overview of these topics.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct. Large language models are trained on raw text data, often sourced from the web. The web is vast, and datasets like Common Crawl are used. However, it's important to be aware that web data can have biases and uneven representation. Some datasets have been analyzed for toxicity, and a small percentage may contain toxic content. Understanding and documenting dataset composition is crucial for responsible AI development.\",\n",
       " 'Scaling laws refer to the relationship between the size of a neural network and its performance. In the context of neural language models, it means that as you increase the size of the model (more parameters), its performance improves. The blog post and paper explore this relationship in more detail.',\n",
       " \"That's a great overview of monitoring machine learning models and gathering user feedback for language models in production. It's important to address issues like latency, incorrect answers, and toxicity. User feedback can be valuable for improving prompts and fine-tuning the model. Test-driven development and behavior-driven development can help in the formal process of developing language models.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'Scaling laws refer to the relationship between the size of a neural network and its performance. In the context of neural language models, it means that as you increase the size of the model (more parameters), its performance improves. The blog post and paper explore this relationship in more detail.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's a great overview of monitoring machine learning models and gathering user feedback for language models in production. It's important to address issues like latency, incorrect answers, and toxicity. User feedback can be valuable for improving prompts and fine-tuning the model. Test-driven development and behavior-driven development can help in the formal process of developing language models.\",\n",
       " \"That's correct! Rate limits are in place to prevent abuse and ensure fair usage. To handle rate limit errors, you can implement strategies like retrying requests with exponential backoff. OpenAI provides guidelines on how to manage rate limits effectively.\",\n",
       " 'LLMOps is a discipline that focuses on improving large language model-powered applications. It involves comparing open source and proprietary models, managing workflows and tools for iteration and prompt management, using test-driven development for LLMs, and choosing the right model for specific use cases.',\n",
       " 'LLMOps is a discipline that focuses on improving large language model-powered applications. It involves comparing open source and proprietary models, managing workflows and tools for iteration and prompt management, using test-driven development for LLMs, and choosing the right model for specific use cases.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's a comprehensive overview of building an effective evaluation set for language models and deploying them in production. It covers techniques like adding diverse examples, generating test cases, continuous data collection, self-critique, test coverage, and gathering feedback. Monitoring performance and user satisfaction is also important. Well done!\",\n",
       " \"That's great! It's important to understand the legal aspects of developing and deploying large language models. Jurisdictional differences, common law, statutory law, and regulatory law all play a role. It's also crucial to consider ethics and norms alongside legal considerations. If you have any specific questions, feel free to ask!\",\n",
       " \"That's great! It's important to understand the legal aspects of developing and deploying large language models. Jurisdictional differences, common law, statutory law, and regulatory law all play a role. It's also crucial to consider ethics and norms alongside legal considerations. If you have any specific questions, feel free to ask!\",\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " 'It sounds like Sergey and Charles are discussing the design principles for Language User Interfaces (LUIs) enabled by Language Models (LLMs). They are likely talking about how to create user-friendly interfaces for AI-powered language-based applications, such as GitHub Copilot and Bing Chat. They emphasize the importance of affordances (clear actions), signifiers (visual cues), mapping (matching user expectations), feedback, and empathy in designing effective LUIs.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " 'It sounds like the blog explores different ways to evaluate the quality of abstractive summaries. It mentions traditional metrics like ROUGE and BERTScore, but also introduces a new approach using Large Language Models (LLMs) like gpt-4 to score the summaries. This can help assess their actual quality more accurately.',\n",
       " 'The DoctorQA system uses a retrieval and reader framework to solve the challenging problem of open domain question answering. It retrieves relevant documents from a large collection and then uses a reader model to find the answer within those documents. This approach helps improve performance in answering questions from a wide range of topics.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " 'Implementing guardrails for LLM (Large Language Models) applications is important for controlling performance. This blog provides examples and trade-offs for input and output guardrails. It emphasizes accuracy, latency, and cost considerations. Fine-tuned models or open-source options can help optimize. Asynchronous implementation reduces latency.',\n",
       " \"It seems like you're summarizing the content of a document related to NLP. That's great! If you have any specific questions about NLP or need help with anything related to AI, feel free to ask.\",\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " 'LLMOps is a discipline that focuses on improving large language model-powered applications. It involves comparing open source and proprietary models, managing workflows and tools for iteration and prompt management, using test-driven development for LLMs, and choosing the right model for specific use cases.',\n",
       " 'LLMOps is a discipline that focuses on improving large language model-powered applications. It involves comparing open source and proprietary models, managing workflows and tools for iteration and prompt management, using test-driven development for LLMs, and choosing the right model for specific use cases.',\n",
       " \"That's great! It's important to understand the legal aspects of developing and deploying large language models. Jurisdictional differences, common law, statutory law, and regulatory law all play a role. It's also crucial to consider ethics and norms alongside legal considerations. If you have any specific questions, feel free to ask!\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'That sounds like a useful blog for financial analysts! The LlamaIndex framework helps extract information from lengthy financial documents like 10-K reports. It provides step-by-step instructions on setup, data loading, indexing, and running queries on the documents. This can help analysts quickly analyze and synthesize insights from financial data.',\n",
       " 'That sounds like a comprehensive video that explores different ways to enhance language models with external context. It discusses techniques like retrieval augmentation, chaining, and tool use to improve the performance of language models. It also covers information retrieval techniques and how to work with the limited context window of language models.',\n",
       " 'LLM SDKs are software development kits that help developers integrate generative AI capabilities into their applications. They provide tools like APIs, sample code, and documentation to make the development process easier and ensure compliance with industry standards. Examples of LLM SDKs include LangChain, LLaMA Index, and LiteLLM, which offer features like connecting data sources, embedding models, and customization options for different use cases.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " 'LLMOps is a discipline that focuses on improving large language model-powered applications. It involves comparing open source and proprietary models, managing workflows and tools for iteration and prompt management, using test-driven development for LLMs, and choosing the right model for specific use cases.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"It seems like you're summarizing a lecture on the harms of large language models (LLMs). These concerns include performance disparities, social biases, toxicity, misinformation, security and privacy risks, copyright and legal protections, environmental impact, and centralization of power. The lecture also emphasizes the importance of considering these harms and draws parallels with other fields like bioethics and food safety.\",\n",
       " 'Building large language model (LLM) applications for production can be challenging due to the lack of engineering rigor in prompt engineering and the ambiguous nature of natural languages. This blog post discusses the challenges, composing multiple tasks with control flows, and promising use cases for LLMs.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'That seems to be about computer security, not AI.',\n",
       " \"That's great! Large Language Models (LLMs) can indeed be used for information extraction. The three approaches you mentioned - tool/function calling mode, JSON mode, and prompting based - are different ways to extract structured output. The quickstart example likely demonstrates how to call a tool or function to extract information. Additional resources can provide guidelines, reference examples, and other extraction tools.\",\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. This helps maximize their potential in various industries.\",\n",
       " 'Implementing guardrails for LLM (Large Language Models) applications is important for managing performance. This blog provides examples and trade-offs for input and output guardrails. It highlights accuracy, latency, and cost considerations, and suggests using fine-tuned models or open-source options for optimization. Asynchronous implementation can reduce latency.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " \"Yes, you're correct! GPT-3 is a language model with a large number of parameters. Scaling up models with more parameters, larger datasets, and increased compute power can improve performance. Avoiding bottlenecks, such as slow data access or limited memory, is important for achieving better performance in AI applications.\",\n",
       " 'Yes, chunking is indeed an important technique in building NLP-related applications. It involves breaking down text into smaller segments, such as phrases or sentences, to analyze and process them more effectively. The optimal chunk size depends on the specific application and its requirements. Different chunking methods and tradeoffs can be considered to achieve the desired results.',\n",
       " \"That's correct! LLMs are indeed powerful, but they have limitations. They can't provide real-time information and are like autocomplete engines. To make the most of LLMs, we need supporting components like prompt engineering and understanding the LLM ecosystem. These components help maximize their potential and drive innovation in various industries.\",\n",
       " 'Access control is indeed crucial in user-facing applications. Implementing access control mechanisms like ACL (Access Control List), RBAC (Role-Based Access Control), ABAC (Attribute-Based Access Control), or ReBAC (Relationship-Based Access Control) outside of the database allows for fine-grained control over user access to indexed documents in RAG applications. It helps ensure that only authorized users can access sensitive information.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followUps_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
