****************************************
CommonTs\src\ReadMe.Salon.md
****************************************
**ActivityRepositoryApi.ts**

The `ActivityRepositoryApi` class extends `Api` and implements `IStorableRepositoryApiWrapper`, providing methods to interact with an activity repository API. This class is initialized with an `IEnvironment` environment and a session key for authentication, and it composes an instance of `StorableRepositoryApi` for underlying API operations.

Key methods include:
- `save(record: IStorable)`: Saves an activity record.
- `remove(recordId: string)`: Removes an activity record.
- `load(recordId: string)`: Loads an activity record.
- `find(functionalSearchKey: string)`: Finds an activity record.
- `recent(querySpec: IStorableMultiQuerySpec)`: Retrieves recent activity records.

These methods construct appropriate API URLs using environment settings and invoke corresponding methods on the `StorableRepositoryApi` instance.

**Api.ts**

This code defines a class, `Api`, that interacts with a specified environment and uses a session key for authentication.

The `Api` class constructor takes two parameters: `environment_`, which is of type `IEnvironment`, and `sessionKey_`, a string utilized for authentication.

The class stores these parameters as private properties `_environment` and `_sessionKey`.

Two public `get` methods, `environment` and `sessionKey`, provide access to these private properties.

This class acts as a base class for more functional API classes by holding common data elements like environment and session key.

Important Classes/Functions:
1. `Api` class
2. `constructor` method
3. `environment` getter
4. `sessionKey` getter

**Asserts.ts**

The module imports `AssertionFailedError` from a local `Errors` file.

The `throwIfUndefined` function checks if a given value `x` is `undefined`. If it is, the function throws an `AssertionFailedError` with the message "Object is undefined."

The `throwIfNull` function checks if a given value `x` is `null`. If it is, it throws an `AssertionFailedError` with the message "Object is null."

The `throwIfFalse` function checks if a given boolean value `x` is `false`. If it is, the function throws an `AssertionFailedError` with the message "Value is false."

Key functions: `throwIfUndefined`, `throwIfNull`, `throwIfFalse`.

**ChunkApi.Types.ts**

The code is for a chunk request API from Braid Technologies Ltd.

The `IChunkRequest` interface defines the properties for a chunk request:
- `text`: The text content that needs to be chunked.
- `chunkSize` (optional): The number of tokens per chunk.
- `overlapWords` (optional): The number of overlapping words between chunks.

The `IChunkResponse` interface defines the return type of the chunk request:
- `chunks`: An array of strings, each representing a chunk of the original text.

**ChunkRepositoryApi.ts**

`ChunkRepostoryApi` extends an `Api` class and implements `IStorableRepostoryApiWrapper`, offering methods to interact with a chunk repository API, including `save`, `remove`, `load`, `find`, and `recent` records. 

The class is initialized with an environment and session key, leveraging `StorableRepostoryApi` for actual API calls. Methods construct API URLs using environment and session key for authentication.

- **save**: Saves a chunk record.
- **remove**: Removes a chunk record by its ID.
- **load**: Loads a chunk record by its ID.
- **find**: Searches for a chunk record.
- **recent**: Retrieves recent records based on query specifications.

Main classes and functions:
- `ChunkRepostoryApi`
- `save`
- `remove`
- `load`
- `find`
- `recent`

**ChunkRepositoryApi.Types.ts**

This code defines type structures and interfaces for a ChunkRepository API developed by Braid Technologies Ltd.

**Important classes/interfaces:**
1. **IStorable**: An imported interface that likely defines a common structure or functionality for storable objects.
2. **IStoredEmbedding**: An interface for storing embeddings containing a `modelId` and an array of numbers representing the embedding.
3. **IStoredTextRendering**: Defines the structure for storing text renderings, including a `modelId` and a `text`.
4. **IStoredChunk**: Extends `IStorable` and represents a chunk of data including properties such as `parentChunkId`, `originalText`, `url`, `storedEmbedding`, `storedSummary`, `storedTitle`, and `relatedChunks`.

**Additional variables:**
- **storedChunkClassName**: A constant string, `"Chunk"`, possibly used for identification or referencing within the system.

**ClassifyApi.Types.ts**

This code defines TypeScript interfaces for a Chunk API as part of Braid Technologies Ltd in 2024.

**IClassifyRequest**: Represents the structure of a classification request object, containing a `text` property of type string and `classifications` which is an array of strings.

**IClassifyResponse**: Represents the structure of a classification response object, containing a single `classification` property of type string.

These interfaces serve as TypeScript definitions to ensure strong typing and clear structure when implementing the API's request and response handling.

**Compress.ts**

This code provides two main functions: `compressString` and `decompressString`. 

The `compressString` function compresses a given input string using the deflate algorithm provided by the `pako` library and returns a base64 encoded string. It handles both Node.js and browser environments for base64 encoding.

The `decompressString` function reverses this process. It takes a base64 encoded compressed string, decodes it appropriately for the environment (Node.js or browser), decompresses it, and returns the original string. It also includes error handling for invalid input. 

Important functions: `compressString` and `decompressString`.

**EmbedApi.Types.ts**

This code defines data structures for an Embed API.

The `IEmbedRequest` interface outlines the structure for embedding requests, containing two properties: `persona` of type `EPromptPersona` and `text` of type `string`.

The `IEmbedResponse` interface defines the structure for embedding responses, with a single property `embedding`, which is an array of numbers.

The important classes or functions in this module are `IEmbedRequest` and `IEmbedResponse`. The code imports `EPromptPersona` from another module to use in the `IEmbedRequest` persona property.

**EnrichedChunk.ts**

The module defines an `EChunkRepository` enumeration to differentiate between various chunk repositories such as 'Boxer' and 'Waterfall'.

The constant `kDefaultSimilarityThreshold` is set to 0.5 to filter out chunks that are less than 50% relevant.

The `IEnrichedChunkSummary` interface represents a summary of a chunk with properties like `url`, `text`, and `summary`.

The `IEnrichedChunk` interface extends `IEnrichedChunkSummary` to include `id` and an array of numbers named `embedding`, meant for server-side storage.

The `IRelevantEnrichedChunk` interface associates a chunk with a relevance score.

`IChunkQuerySpec` specifies the structure for chunk query objects, including `repositoryId`, `maxCount`, and `similarityThreshold`.

The `IChunkQueryRelevantToUrlSpec` and `IChunkQueryRelevantToSummarySpec` interfaces extend `IChunkQuerySpec` to include `url` and `summary` properties respectively.

**EnrichedQuery.ts**

The module provides definitions for the data elements of the Query API.

It imports the `EChunkRepository` and `IRelevantEnrichedChunk` classes from the `EnrichedChunk` module.

The `EConversationRole` enum defines different roles within a conversation element: `kSystem`, `kAssistant`, and `kUser`.

The `EStandardPrompts` enum includes predefined prompts for different scenarios in an AI assistant, helping an application developer understand generative AI with strict guidelines on responses.

The `IConversationElement` interface defines the structure for a conversation element with a role and content.

The `IEnrichedQuery` interface outlines the structure for an enriched query object, including repository information, prompts, and history.

The `IEnrichedResponse` interface defines the structure for an enriched response object with `answer` and `chunks`.

The `IGenerateQuestionQuery` interface defines the structure for a question generation query.

The `IQuestionGenerationResponse` interface defines the structure for the response to a generated question query.

**EnumerateModelsApi.Types.ts**

This code defines TypeScript interfaces for the EnumerateModels API, owned by Braid Technologies Ltd, for the year 2024.

The `IEnumerateModelsRequest` interface represents the request object for fetching models, while the `IEnumerateModelsResponse` interface represents the response object, containing properties like defaultId, defaultEmbeddingId, largeId, largeEmbeddingId, smallId, and smallEmbeddingId.

Similarly, `IEnumerateRepositoriesRequest` defines the request object for fetching repositories, and the `IEnumerateReposotoriesResponse` interface represents the response, containing an array of `EChunkRepository` objects.

The `EChunkRepository` is imported from an external module `EnrichedChunk`.

**Environment.ts**

This code defines classes for the DevelopmentEnvironment, StagingEnvironment, and ProductionEnvironment, each implementing the IEnvironment interface.

Each class includes methods to retrieve various API endpoints specific to each environment. For example, the DevelopmentEnvironment class uses localhost URLs, while the StagingEnvironment and ProductionEnvironment use URLs pointing to a hosted API service.

The important classes in this module are DevelopmentEnvironment, StagingEnvironment, and ProductionEnvironment.

Key functions include checkSessionApi, summariseApi, findThemeApi, classifyApi, chunkApi, embedApi, and others, each returning the corresponding API endpoint URL. There is also a boxerHome function returning the URL of a boxer HTML page, among other endpoint functions.

**Errors.ts**

This module defines custom error classes representing different types of application errors. The main classes are `InvalidParameterError`, `InvalidOperationError`, `InvalidStateError`, `ConnectionError`, `EnvironmentError`, and `AssertionFailedError`.

Each class extends the built-in `Error` class and takes an optional message parameter. They set their prototype chain correctly using `Object.setPrototypeOf` to ensure stack traces are accurate. The class names are also set for proper display in stack traces.

Logging functions (`logCoreError` and `logApiError`) are called within each constructor to log the error details, aiding in debugging and error tracking.

**FindEnrichedChunkApi.ts**

The `FindEnrichedChunkApi` class extends the `Api` class to provide methods for finding enriched chunks. 

The constructor initializes the class with an environment and a session key. 

The `findChunkFromUrl` function takes a URL query and returns an enriched chunk summary if available. It makes an asynchronous POST request to an API endpoint and handles the response.

The `findRelevantChunksFromUrl` function accepts a URL query and returns an array of relevant enriched chunks by making an asynchronous POST request to another API endpoint.

The `findRelevantChunksFromSummary` function takes a summary query and returns relevant enriched chunks, similar to the previous methods. 

Important functions include `findChunkFromUrl`, `findRelevantChunksFromUrl`, and `findRelevantChunksFromSummary`.

**FindThemeApi.Types.ts**

This code defines TypeScript interfaces for a FindTheme API used by Braid Technologies Ltd. 

The `IFindThemeRequest` interface represents the request object that includes two properties: `text`, a string containing the input data, and `length`, a number indicating the length of the text. 

The `IFindThemeResponse` interface represents the response object with a single property: `theme`, a string representing the identified theme from the input text.

Key classes/interfaces: `IFindThemeRequest`, `IFindThemeResponse`.

**Fluid.ts**

This code defines TypeScript interfaces for the Fluid Token API.

The `IFluidUser` interface represents a user and includes properties for `local` (indicating if the application runs locally), `userId` (user's ID), and `userName` (user's name).

The `IFluidTokenRequest` interface extends `IFluidUser` and adds the `documentId` property to represent the request for a Fluid token. This includes all properties from `IFluidUser` along with the document ID for which the token is requested.

The `IFluidTokenResponse` interface represents the response to a Fluid token request and contains a single property `token`, which is the token itself.

Important classes or functions: `IFluidUser`, `IFluidTokenRequest`, `IFluidTokenResponse`.

**FluidApi.ts**

**Important Classes/Functions:**
1. `FluidApi`
2. `generateToken`

The `FluidApi` class extends the `Api` class and is responsible for interacting with an API related to fluid tokens. 

The constructor (`constructor`) initializes an instance with the provided `IEnvironment` and session key for authentication.

The `generateToken` method asynchronously generates a token using the provided query parameters, making a POST request to the API. 

Retries up to 5 times are managed using the `axiosRetry` module configured with exponential backoff for retrying on specific conditions.

 Error handling is incorporated to log any issues and ensure undefined is returned if the process is unsuccessful.

**FluidTokenProvider.ts**

The code provides an implementation of an API for establishing Fluid connections, specifically using Azure.

The `FluidTokenProvider` class implements the `ITokenProvider` interface to retrieve tokens required for connecting to Azure Fluid Relay. It uses the `FluidApi` class to generate tokens and manages user details with the `IFluidUser` interface. 

The `FluidConnectionConfig` class implements `AzureRemoteConnectionConfig` and is responsible for setting up connection configurations, such as endpoint, tenant ID, and token provider, based on the specified environment and session key.

The `FluidClientProps` class implements `AzureClientProps`, and it initializes a new Fluid connection using `FluidConnectionConfig`, based on the given session key, token request, and environment settings.

Key classes:
- `FluidTokenProvider`
- `FluidConnectionConfig`
- `FluidClientProps`

Important functions:
- `FluidTokenProvider.fetchOrdererToken()`
- `FluidTokenProvider.fetchStorageToken()`
- `FluidTokenProvider.getToken()`

**IEnvironment.ts**

The code defines a constant `BRAID_ENVIRONMENT_KEY` with the value "BRAID_ENVIRONMENT".

It declares an enumeration `EEnvironment` with values `kLocal`, `kStaging`, and `kProduction`, representing different environments.

An interface `IEnvironment` is defined with multiple method signatures related to API interactions. Each method returns a string and appears to represent different API endpoint paths and operations, such as checking sessions, summarization, finding themes, and handling activities, chunks, and pages.

Classes and functions:
1. `BRAID_ENVIRONMENT_KEY` (constant)
2. `EEnvironment` (enum)
3. `IEnvironment` (interface with various methods)

These components support environment configuration and interactions with various backend API endpoints in a scalable manner.

**IEnvironmentFactory.ts**

This module manages the selection of environment configurations for an application.

- The main classes and functions are `getDefaultEnvironment`, `getDefaultFluidEnvironment`, `getDefaultLoginEnvironment`, and `getEnvironment`.
- `getDefaultEnvironment` returns an environment instance based on the current execution context. It defaults to `DevelopmentEnvironment` if the `BRAID_ENVIRONMENT` environment variable is set to 'Local', otherwise defaults to `ProductionEnvironment`.
- `getDefaultFluidEnvironment` and `getDefaultLoginEnvironment` functions work similarly but prioritize `DevelopmentEnvironment` if running in a browser on `localhost`.
- `getEnvironment` function returns the environment instance (`DevelopmentEnvironment`, `StagingEnvironment`, or `ProductionEnvironment`) based on the string passed to it.

**IModel.ts**

This code defines an enumeration `EModel` with two possible values (`kSmall` and `kLarge`) representing different model sizes.

It also defines an interface `IModel` which outlines the structure that a model object should follow. This interface includes properties for deployment details (`deploymentName`, `embeddingDeploymentName`) and `contextWindowSize`.

The interface `IModel` also requires implementing three methods: `fitsInContext()` to check if the given text fits within the context window size, `chunkText()` to split text into chunks considering chunk size and overlap, and `estimateTokens()` to estimate the number of tokens in the text.

Important entities:
- `EModel` (enum)
- `IModel` (interface)

**IModelFactory.ts**

This code provides functionality to retrieve AI models, primarily using instances of the `GPT4` class.

The `getDefaultModel` function returns a default instance of the `GPT4` model.

The `getModel` function returns an instance of a specific model based on an `EModel` type, currently defaulting to return `GPT4`.

Key classes and functions include:
- `GPT4`: Represents the model class being instantiated and returned.
- `getDefaultModel`: Fetches the default `GPT4` model.
- `getModel`: Fetches the specified model based on the `EModel` type, with the default also being `GPT4`.

**IPromptPersona.ts**

This code defines an enumeration and an interface used primarily for summarisation tasks by different personas. 

**Classes and Functions:**
1. **EPromptPersona:** An enumeration that lists different types of personas (e.g. Article Summariser, Code Summariser, Survey Summariser). Each persona corresponds to a specific summarisation functionality.
2. **IPromptPersona:** An interface that outlines the structure of a summarisation persona. It contains three properties: `name` (a string representing the persona's name), `systemPrompt` (a string with the system-level prompt), and `itemPrompt` (a string with the prompt used for individual items).

These components facilitate organized and flexible summarisation by categorizing and structuring the personas and their prompts.

**IStorable.ts**

The code defines several TypeScript enums and interfaces that are designed to manage and interact with storable objects within an application context.

The `EStorableApplicationIds` enum represents the names of the applications (`Boxer` and `Waterfall`) that can generate and use stored objects.

The `IStorable` interface outlines the structure of storable objects, including properties such as `id`, `applicationId`, `contextId`, `userId`, `functionalSearchKey`, `created`, `amended`, `className`, and `schemaVersion`.

The `IStorableMultiQuerySpec` interface defines a specification for querying multiple records, specifying the `limit` and `className` of the records to return.

The `IStorableQuerySpec` interface defines a specification for querying a single record, specifying the `id` and optionally a `functionalSearchKey`.

The `IStorableOperationResult` interface indicates whether an operation on a storable object succeeded with a boolean `ok` property.

**Logging.ts**

This module provides four functions to log different types of messages.

The `logCoreError` function logs core errors with a given description and additional details using `console.error`.

The `logDbError` function logs database-related errors using `console.error` with a specified description and extra details.

The `logApiError` function logs API-specific errors with a provided description and details using `console.error`.

The `logApiInfo` function logs general information related to APIs using `console.log`, specifying a description and relevant details.

Key functions in the module are `logCoreError`, `logDbError`, `logApiError`, and `logApiInfo`.

**LoginApi.ts**

This code defines a `LoginApi` class for handling login operations, specifically with LinkedIn API, by extending an `Api` base class.

The `LoginApi` class constructor initializes an instance with the environment settings and a session key for authentication, which it passes to the `Api` base class.

The `login` method constructs the API URL using the LinkedIn login API and the session key. It then makes a POST request to the URL using axios. If the response status is 200, it returns "Redirecting...". Otherwise, it logs the error status or error message and returns an empty string.

Key Class: `LoginApi`
Key Method: `login`

**Model.ts**

The module defines a `GPT4` class that implements the `IModel` interface, simulating a model with specific deployment settings and context window sizes used for handling textual data.

The class constructor initializes four properties: `deploymentName`, `embeddingDeploymentName`, `contextWindowSize`, and `contextWindowSizeWithBuffer`.

The `fitsInContext` method checks whether a given text fits within the predefined context window size with a buffer, using the `GPT4Tokenizer` to estimate token count.

The `chunkText` method splits input text into chunks based on a specified chunk size and optional overlapping words, throwing an error if the overlap exceeds the chunk size.

The `estimateTokens` method estimates the number of tokens in a text using the tokenizer.

**Important classes/functions**: `GPT4` class, `fitsInContext`, `chunkText`, `estimateTokens` functions.

**PageRepositoryApi.ts**

**Important Classes/Functions:**

- `PageRepostoryApi`: Represents an API for the Page repository, extends `Api` and implements `IStorablePageRepostoryApiWrapper`.
- `save`: Asynchronously saves a record to the Page repository API.
- `compressString`: Compresses a string using the deflate algorithm and returns a Base64-encoded compressed string.
- `decompressString`: Decompresses a string that was compressed using `compressString` and returns the original string.

**Summary:**

The `PageRepostoryApi` class extends the `Api` class and implements the `IStorablePageRepostoryApiWrapper`. It takes an environment and a session key as parameters for initialization. The class includes a method `save` which saves a record that implements `IStorable` to the Page repository API asynchronously. Additionally, it provides methods `compressString` and `decompressString` for compressing and decompressing strings respectively, using the deflate algorithm.

**PageRepositoryApi.Types.ts**

This TypeScript module defines interfaces relevant to the `PageRepository` API, which deals with storing web pages.

The `IStoredPage` interface extends `IStorable` and represents a stored web page, containing the HTML content as a string.

The `IStoredPageRequest` interface extends `IStorableQuerySpec` and represents the request type for fetching or querying stored web pages.

The `IStoredPageResponse` interface extends `IStoredPage` and represents the response type for providing stored web page details.

These explicit type definitions allow code generators to identify and generate test code for both the input and output of page-related API operations.

**QueryModelApi.ts**

This code defines a `QueryModelApi` class that extends from `Api` and interact with a specified environment to query models and generate questions. It uses the axios library to make HTTP requests.

The constructor method initializes a new instance of the class with the provided environment settings and session key for authentication.

The `queryModelWithEnrichment` function asynchronously sends enriched query data to a specified API endpoint and returns the enriched response data if the request is successful, otherwise it logs the error and returns `undefined`.

The `generateQuestion` function asynchronously generates a question based on the provided query data, following a similar pattern to handle responses and errors. 

Key classes and functions:
- `QueryModelApi` class
- `constructor`
- `queryModelWithEnrichment`
- `generateQuestion`

**SessionApi.ts**

The `SessionApi` class extends the `Api` class to handle session-related tasks.

In the constructor, it accepts an environment configuration (of type `IEnvironment`) and a session key, which are passed to the parent `Api` class's constructor.

The `checkSessionKey` method sends a POST request to validate the session key using `axios`. It constructs the endpoint URL by appending the session key. If the response status is 200, it returns the response data, otherwise, it logs the error status and returns an empty string.

Important classes or functions:
- `SessionApi`
- `constructor`
- `checkSessionKey`

**StorableRepositoryApi.ts**

The code defines an interface and a class for interacting with a repository of storable objects using Axios for HTTP requests.

`IStorablePageRepostoryApiWrapper` interface provides a method `save(record: IStorable): Promise<boolean>` to save storable records.

`IStorableRepostoryApiWrapper` interface extends the previous one, adding methods `remove(recordId: string): Promise<boolean>`, `load(recordId: string): Promise<IStorable | undefined>`, `find(functionalSearchKey: string): Promise<IStorable | undefined>`, and `recent(querySpec: IStorablesQuerySpec, url: string): Promise<Array<IStorable>>`.

`StorableRepostoryApi` class implements methods to handle API calls: `save`, `remove`, `load`, `find`, and `recent`.

`save` sends a POST request to save a record and returns true if successful.

`remove` sends a POST request to delete a record by ID and returns a boolean status.

`load` sends a POST request to retrieve a record by ID and returns the record or undefined.

`find` sends a POST request to find a record using a functional search key and returns the record.

`recent` sends a POST request with query specifications to retrieve recent records and returns an array of records.

**StudioApi.Types.ts**

This code defines the data structures for the Studio API by declaring interfaces for request and response objects.

The `IStudioBoxerRequest` interface represents a request object that contains a single property `question` of type `string`.

The `IStudioBoxerResponseEnrichment` interface represents a response object with several properties: `id` (string), `summary` (string), and optional `title`, `url`, and `iconUrl` (all strings, may be undefined).

Important interfaces: `IStudioBoxerRequest`, `IStudioBoxerResponseEnrichment`.

**SummariseApi.Types.ts**

This code defines the data structures for the Summarise API.

There is an `ISummariseRequest` interface that specifies the format for a summarisation request, including properties for `persona` (of type `EPromptPersona`), `text` (the content to be summarised), and an optional `lengthInWords`.

There is an `ISummariseResponse` interface which represents the structure for the response of the summarisation request, containing a single property, `summary`, which is a string.

The `EPromptPersona` is imported from another module, indicating that it defines various personas that can be used in the request.

Key interfaces: `ISummariseRequest` and `ISummariseResponse`.

**TestForSummariseFailApi.Types.ts**

This code defines the data elements for the `SuppressSummariseFail` API.

The `ITestForSummariseFailRequest` interface specifies the structure of a request object with a required `text` property and an optional `lengthInWords` property.

The `ETestForSummariseFail` enum contains two values: `kSummaryFailed`, indicating that a summary attempt failed, and `kSummarySucceeded`, indicating that a summary attempt was successful.

The `ITestForSummariseFailResponse` interface specifies the structure of a response object with an `isValidSummary` property, which uses the `ETestForSummariseFail` enum to indicate whether the summary was successful or not.

**ThemeApi.ts**

This code is part of the FindTheme API developed by Braid Technologies Ltd in 2024.

It defines an interface named `IFindThemeRequest`.

The `IFindThemeRequest` interface contains two properties: a `text` property of type `string` and a `length` property of type `number`.

The interface is used for specifying the criteria needed to find a theme within the API.
****************************************

****************************************
Salon\apis\ReadMe.Salon.md
****************************************
**FindThemeApi.Types_test.py**

This code defines JSON Schemas for request and response data structures (`IFindThemeRequestSchema` and `IFindThemeResponseSchema`). These schemas specify required properties, data types, and prohibit additional properties.

The `test_valid_ifind_theme_request`, `test_invalid_ifind_theme_request_missing_text`, `test_invalid_ifind_theme_request_missing_length`, `test_invalid_ifind_theme_request_additional_properties`, and `test_invalid_ifind_theme_request_wrong_type` functions define test cases for validating instances against `IFindThemeRequestSchema`.

Similarly, the `test_valid_ifind_theme_response`, `test_invalid_ifind_theme_response_missing_theme`, `test_invalid_ifind_theme_response_additional_properties`, and `test_invalid_ifind_theme_response_wrong_type` functions define test cases for validating instances against `IFindThemeResponseSchema`.

These tests utilize `pytest` and `jsonschema` libraries to ensure correct schema implementation and validate payload correctness.

**PagerepositoryApi.Types_test.py**

This module provides unit tests for a web API using the `pytest` framework and mocks HTTP requests with `unittest.mock.patch`.

**Important functions:**
- `test_get_page_success()`: This function tests if an API call to fetch a page with the given URL and parameters returns a successful response (status code 200) and the expected HTML content.
- `test_get_page_missing_param()`: This function tests if an API call without required parameters returns a 400 status code and an appropriate error message indicating missing parameters.

`BASE_URL` is a constant holding the base URL for the API, and `sample_successful_html` provides sample HTML for a successful test response.
****************************************

****************************************
Salon\src\ReadMe.Salon.md
****************************************
**api_to_test_code.py**

This script generates Pytest code from API data provided in JSON or YAML format using OpenAI's API.

**Key functions and classes:**
- `parse_arguments()`: Parses command-line arguments to get the input file path.
- `extract_code(content)`: Extracts Python code snippets from the provided content using predefined markers.
- `load_api_data(file_path)`: Loads and returns API data from a JSON or YAML file.
- `main()`: Orchestrates the script by parsing arguments, loading API data, generating test code using OpenAI, and saving the output.

**Additional components:**
- Configures logging to capture different log levels.
- Utilizes OpenAI client with API key.
- Handles JSON and YAML file formats.
- Error handling for file operations, JSON/YAML parsing, and OpenAI API interactions.

**repo_to_text.py**

This script processes a local GitHub repository, concatenating files into text files within set word limits and summarizing source files. Users can specify configurations, repository paths, output directories, and files or directories to skip via command-line arguments.

**Key Classes/Functions:**

- **RepoContentProcessor**: This is the main class responsible for processing repository files. 
  - **`__init__()`**: Initializes with repository and configuration paths.
  - **`make_common_file_name()`**: Generates a common file name format.
  - **`format_file_block()`**: Formats a block of file content.
  - **`count_words()`**: Counts words in a file.
  - **`process_file()`**: Reads and processes individual files.
  - **`process_repo()`**: Walks through the repository and processes files.
  - **`save_current_content()`**, **`save_directory_content()`**: Save accumulated contents to output files.

- **SummarisedDirectory**: Stores directory names and their summaries.

- **load_yaml()**: Loads the configuration file.

- **summarise_code()**: Summarizes code content via an endpoint.

- **parse_arguments()**, **validate_args()**: Parse and validate command-line arguments.

The script respects word limits and allows skipping irrelevant files/directories. The construct `if __name__ == "__main__":` ensures `main()` runs only when the script is executed directly, not imported. The return `1` signifies an error or non-successful conclusion, meaning new developers should check the `main()` function for full context.
****************************************

****************************************
Waterfall\src\ReadMe.Salon.md
****************************************
**boxer_pipeline.py**

The provided code sets up and runs a BoxerDataPipeline class to search, download, process, and store data from YouTube and HTML sources.

The **BoxerDataPipeline** class is initialized with an output location where processed data will be stored.

The main **search** method in this class takes specifications for YouTube and HTML data (YouTubePipelineSpec, HtmlDirectedPipelineSpec, and PipelineFileSpec). It performs the following tasks: 
- Searches for YouTube playlists and transcripts using **YoutubePlaylistSearcher** and **YouTubeTranscriptDownloader**.
- Breaks down YouTube transcripts into chunks via **YouTubeTranscriptChunker**.
- Crawls and downloads HTML links with **HtmlLinkCrawler** and **HtmlFileDownloader**.
- Summarises the content using the **Summariser** class.
- Embeds the summarised content using the **Embedder** class.

Processed data is enriched and saved using the **save_chunks** function. The results are then saved to a JSON file.

**boxer_sources.py**

This code module maintains a list of curated educational resources for AI/ML.

It provides YouTube playlists on machine learning, NLP, and AI fundamentals, which include courses taught by prominent experts like Andrew Ng.

It also lists crucial articles and tutorials from industry experts, as well as documentation from leading AI platforms.

Some notable resources include those from Pinecone, articles by Andrej Karpathy, practical courses from fast.ai, and various resources on transformers and deep learning.

These materials are intended to help users understand core AI/ML concepts and keep up with the latest developments in the field.

**chunker.py**

### Summary

**Classes:**
1. `Chunker`: Inherits from `PipelineStep` to segment text into smaller parts.

**Functions:**
1. `__init__(self, output_location: str)`: Initializes `Chunker` with the given output location.
2. `chunk(self, pipeline_item: PipelineItem, chunk_size_words: int, overlap_words: int) -> list[PipelineItem]`: Splits text into smaller segments using an external API.

### Key Points

- The code uses the `Chunker` class to split a text string into smaller segments.
- Initializes logging and sets the log level to `WARNING`.
- Uses a session key retrieved from environment variables for API authentication.
- Requests module is configured with retries for robustness.
- Sends a POST request to an API endpoint to perform the chunking.
- If the API call is successful, it processes the response and returns a list of `PipelineItem` objects representing the chunks.


**cluster_analyser.py**

The code defines a class `ClusterAnalyser` that inherits from `PipelineStep`. 

The `__init__` method initializes the `ClusterAnalyser` with an output location and the number of clusters for the KMeans algorithm.

The `analyse` method performs KMeans clustering on the embedding vectors of a list of `PipelineItem` objects. It extracts the embeddings from items, fits the KMeans model to them, assigns cluster labels to each item, and returns the updated items.

Important classes and functions:
- `ClusterAnalyser`
- `__init__`
- `analyse`

**db_repository.py**

This Python code module is designed for storing data in the Chunk table of the BraidApis database. It handles data in the form of 'PipelineItem' objects and converts them into 'Chunk' objects to be passed to the native Chunk API, which is used across various applications.

The primary class is `DbRepository`, which provides methods for loading, saving, and checking the existence of files in the Braid Cosmos database. The important methods in this class are `save`, `find`, and `exists`.

It utilizes logging for script execution tracking, and key libraries include `uuid` for generating unique IDs and `datetime` for timestamp handling. The implementation also leverages classes from `chunk_repository_api` for handling chunk-related operations.

**embedder.py**

The code defines an `Embedder` class, inheriting from `PipelineStep`, which creates text string embeddings.

The `Embedder` class contains:
- `__init__`: Initializes with the output location.
- `embed`: Generates embeddings for a `PipelineItem`. If an embedding exists, it loads from the repository; otherwise, it creates a new embedding using an external API, saves it, and returns the updated `PipelineItem`.
- `embed_text`: Generates an embedding for a given text string using an external API and returns the embedding as a list of floats.

The code sets up logging configurations and includes retry mechanisms for handling HTTP requests.

**embedder_repository_facade.py**

This code provides a facade for storing embeddings in a local file system.

The `read_file_names` function retrieves a list of filenames matching a specified pattern within a given directory. 

The `EmbeddingRespositoryFacade` class offers an interface to load, save, and check the existence of files. 

Important methods in this class include:
- `__init__`: Initializes the class with a specified output location and sets up a `FileRepository` instance.
- `spec`: Returns the file extension pattern for stored files.
- `list_contents`: Lists content by retrieving and stripping off extensions from the filenames.
- `save`: Saves an embedding into a file at a specified path.
- `load`: Loads and returns the contents of a specified file as a list of floats.
- `exists`: Checks if a file exists at a specified path.
- `text_to_float`: Converts a string of numbers into a list of floats.

**embedding_finder.py**

The module computes the nearest embedding to a target text using cosine similarity. 

`cosine_similarity(a, b)` function computes the cosine similarity between two vectors `a` and `b`.

The `EmbeddingFinder` class initializes with a list of embeddings and an output location. It uses the `find_nearest` method to determine the nearest embedding to a given target text string. This method creates a `PipelineItem`, embeds the target text using the `Embedder` class, then iterates through the embeddings to find the one with the highest cosine similarity to the target text.

The important classes and functions are:
- `cosine_similarity`
- `EmbeddingFinder`
- `find_nearest`

**file_repository.py**

This Python module is designed to store data in the local file system. 

The module imports standard libraries for logging (`logging`), interacting with the operating system (`os`), and handling JSON data (`json`). It also imports a custom function `make_local_file_path` to process file paths.

Logging is configured to capture warnings and above, with a specified format to include the timestamp, logger name, log level, and message.

The `strip_quotes` function removes all single and double quotes from a given input string.

The `FileRespository` class provides methods for saving, loading, and checking the existence of files within a specified output location:
- `__init__`: Initializes the repository with a designated output location.
- `save`: Saves text content to a file identified by the given path and extension.
- `load`: Loads and returns the content of a file, if it exists, stripping any quotes.
- `exists`: Checks if a file exists at the specified path and extension.

**google_office_mailer.py**

This script uses the Google Gmail API to send emails with attachments. 

The `send_mail` function initiates the process by handling Google OAuth 2.0 authorization, either fetching stored credentials from `token.json` or executing an authorization flow to generate new credentials, which it then saves.

The `send_message_with_attachment` function constructs the email, adding the body and any attachments. It encodes the message using base64 and sends it through the Gmail API.

The `build_file_part` function assists in creating a MIME part for file attachments by guessing the MIME type and packaging the file accordingly.

Important classes or functions:
1. `send_mail`
2. `send_message_with_attachment`
3. `build_file_part`

**html_file_downloader.py**

The code defines a class `HtmlFileDownloader` that extends `PipelineStep` to download the text of a web page and save it locally.

It uses Selenium WebDriver to fetch the content of web pages, handling JavaScript more effectively than requests.

BeautifulSoup is used to parse the HTML content and extract the text.

Logging is set up to capture and display script execution details, with warnings as the default level.

The `download` method accepts a `PipelineItem`, checks if the HTML content already exists in the repository, and fetches and processes it if not.

Headers for HTTP requests are defined but not used, as the script switched from using the requests library.

**html_link_crawler.py**

The script defines a `HtmlLinkCrawler` class, inheriting from `PipelineStep`, which crawls a web page to extract and generate sub-links up to a specified depth. It performs this by parsing HTML content using BeautifulSoup and handling HTTP requests with custom headers to mimic a web browser.

The `crawl` method starts the crawling process, while `crawl_links_recursively` explores each link and collects sub-links recursively, ensuring it doesn't exceed the maximum depth or revisit already crawled links.

Utility functions such as `find_matching_entry`, `deduplicate`, `remove_exits`, `add_prefix`, and `make_fully_qualified_path` are used to manage URLs, remove duplicates, filter out external links, and construct fully qualified URLs, respectively.

**make_local_file_path.py**

This module is designed to create a local file system path from an HTTP URL.

The `make_local_file_path` function converts a given URL into a fake file name by sanitizing it, replacing certain characters (like slashes and query string delimiters) with underscores to ensure compatibility with file system conventions.

The URL is parsed using the `urlsplit` function from the `urllib.parse` module to extract components like scheme, network location, path, and query. These components are concatenated, and then specific characters are replaced.

The resulting string (fake file name) is truncated to a maximum length of 200 characters to ensure it is a valid file name.

**summariser.py**

This code defines a class `Summariser` that inherits from `PipelineStep`. The primary function of the `Summariser` class is to create a summary for a given text string.

The `__init__` method initializes the `Summariser` with an output location. 

The `summarise` method first checks if an existing summary is already saved using the `SummaryRepositoryFacade`. If found, it loads and returns the existing summary. If not, it generates a new summary by making a POST request to an external API, saves the summary, and assigns it to the `PipelineItem`.

Important Classes/Functions:
- `Summariser` class
- `__init__` method
- `summarise` method

**summarise_fail_suppressor.py**

The script defines a `PipelineStep` to create a summary for a text string.

The important classes and functions include:
- `SummariseFailSuppressor`: A `PipelineStep` subclass aimed at creating summaries for text strings and suppressing any items that fail validation checks.
- `__init__`: Initializes the `SummariseFailSuppressor` object with a specified output location.
- `should_suppress`: Evaluates whether a given `PipelineItem` should be suppressed. It makes a POST request to an API endpoint to check the validity of the summary. The decision to suppress or keep the item depends on the API response.

The script establishes a logging configuration for debugging. It uses the `requests` library to handle HTTP requests and retries.

**summary_repository_facade.py**

The `SummaryRepositoryFacade` class provides an interface for file operations in the local file system, specifically tailored for handling summary files with the extension `.summary.txt`.

The `__init__` function initializes the class by creating an instance of `FileRepository` with the specified output location and sets the file extension to `summary.txt`.

The `spec` static method returns the string pattern "*.summary.txt" to filter files by extension.

The `save` method saves the given text to a specified path within the predefined output location using `FileRepository`.

The `load` method reads and returns content from a specified file; it returns an empty string if the file does not exist.

The `exists` method checks whether a specified file exists in the output location, returning True if it does and False otherwise.

**text_repository_facade.py**

This module provides a facade for storing and retrieving text files in the local file system. The primary class is `TextRespositoryFacade`, which includes methods to save, load, and check the existence of text files.

The `__init__` method initializes the class with an `output_location` and sets the file extension to `.txt`. 

The `spec` method returns the file pattern `*.txt`.

The `save` method saves text to the specified path within the output location.

The `load` method loads text from a file at the specified path, returning its contents as a string, or an empty string if the file doesn't exist.

The `exists` method checks whether a file exists at the specified path.

**theme_finder.py**

**Classes and Functions:**
- **ThemeFinder**: Class to create a theme for input text paragraphs.
- **find_theme**: Method within `ThemeFinder` to find a theme by sending text to an external API.

**Code Summary:**
This code provides a `ThemeFinder` class to identify themes for given paragraphs by calling an external API. The class initializes with no specific settings. The `find_theme` method sends a POST request with input text and desired theme length to the API, using a session configured with retries for robustness against server errors. Success responses return the extracted theme, while failures log errors and return None. The script uses standard libraries for logging and HTTP requests and includes headers for user agent and content type in the requests. 

**waterfall_pipeline.py**

This code implements a data processing pipeline that involves searching, downloading, clustering, and analyzing web data to generate thematic reports. The `WaterfallDataPipeline` class is central, with methods such as `search_dynamic` and `search_static` for initiating the process with dynamic or static data sources.

Key functions:
1. `sort_array_by_another`: Orders one list based on another.
2. `make_path`: Concatenates directory and filename.
3. `load_file`: Reads file contents, handling errors appropriately.
4. `create_themes`: Generates themes from clustered items.
5. `create_report`: Compiles a report from items and themes.

Important classes/functions in the module:
- `WebSearcher`
- `HtmlFileDownloader`
- `Summariser`
- `Embedder`
- `ClusterAnalyser`
- `ThemeFinder`
- `EmbeddingFinder`

**waterfall_pipeline_report.py**

**Important Classes/Functions:**
- `create_mail_report(output_location: str, items: list[PipelineItem], themes: list[Theme], spec: WebSearchPipelineSpec, send_final: bool) -> None`

**Summary:**
This code generates and sends an automated Waterfall report via email to the Braid Leadership. The `create_mail_report` function takes output location, a list of `PipelineItem` and `Theme` objects, a `WebSearchPipelineSpec` object, and a boolean `send_final` parameter to manage whether the report should be sent. 

The function compiles a summary which includes an introduction, a brief about the top clusters, and a confidentiality notice. If `send_final` is true, it sends the summary via email using the `send_mail` function from the `src.google_office_mailer` module. Logging is utilized for debugging purposes.

**waterfall_pipeline_report_common.py**

The code is designed to generate a report and save it in both HTML and JSON formats. 

It imports necessary libraries including logging for information display, OS for file operations, JSON for data serialization, Plotly for creating visualizations, and UMAP for dimensionality reduction.

Important classes and functions:
1. `PipelineItem`
2. `Theme`
3. `WebSearchPipelineSpec`
4. `write_chart(output_location, items, themes, spec)`: This function reduces item embeddings to 2D using UMAP, creates a scatter plot with Plotly, and saves the chart as an HTML file.
5. `write_details_json(output_location, items, themes, spec)`: This function creates a detailed JSON file containing data about each PipelineItem for manual inspection.

**waterfall_pipeline_save_chunks.py**

This script facilitates sending a final Waterfall report to a database as chunks. 

### Important Classes and Functions:
- **set_timestamps(chunk, existing)**: Updates the timestamps of a chunk based on its status (new or existing).
- **create_theme_chunk(short_description, context, long_description, output_location, chunk_repository)**: Creates or retrieves a theme chunk and updates its properties.
- **save_chunks(items, spec)**: Saves a list of PipelineItem objects as chunks in the database.
- **save_chunk_tree(output_location, items, themes, spec)**: Saves chunks in a hierarchical structure based on provided pipeline items, themes, and specifications.

### Other Important Components:
- **ChunkRepository**: Manages data access for chunks.
- **DbRepository**: Manages data access for the database.
- **Embedder**: Generates text embeddings.
- **PageRepository**: Manages data access for pages.

### Logging:
- Configures logging to display errors and debug information during execution.

**waterfall_survey_pipeline.py**

The provided code implements a data pipeline using the `WaterfallDataPipeline` class. This pipeline performs web searches, downloads HTML content, summarizes it, and clusters the content into themes, outputting organized themes and reports.

1. **Classes and Functions**:
   - `WaterfallDataPipeline`: Main class implementing the data pipeline.
   - `sort_array_by_another`: Function for sorting an array based on another array.
   - `search`: Initiates the web search and processing pipeline.
   - `search_and_cluster`: Conducts the web search, downloads content, summarizes it, checks for summarization failures, embeds content, and clusters it.
   - `create_themes`: Creates themes from the clustered content.
   - `create_report`: Generates and stores a report based on processed data.

2. **Pipeline Stages**:
   - **Search**: Uses `WebSearcher` to fetch initial links.
   - **Download and Summarize**: Downloads HTML using `HtmlFileDownloader` and summarizes content using `Summariser`.
   - **Error Handling**: Uses `SummariseFailSuppressor` to manage summarization failures.
   - **Embedding**: Embeds the summarized content using `Embedder`.
   - **Clustering**: Utilizes `ClusterAnalyser` to categorize content into clusters.
   
3. **Theme Creation**:
   - Aggregates summaries and categorizes them into themes using `ThemeFinder`.
   - Enriches themes with nearest relevant articles using `EmbeddingFinder`.

4. **Report Generation**:
   - Generates comprehensive reports, including details and chunk trees, and sends email reports through `create_mail_report`.

Key modules include `WebSearcher`, `HtmlFileDownloader`, `Summariser`, `Embedder`, `ClusterAnalyser`, `ThemeFinder`, `EmbeddingFinder`, and reporting functions in `waterfall_pipeline_report` and `waterfall_pipeline_save_chunks`. Logging is set up to capture warnings and higher-level messages.

**web_searcher.py**

This Python script sets up a web search utility as the first step in a Waterfall pipeline. It uses the Google Custom Search Engine API to find relevant URLs for a given query. The script is designed for use with Braid Technologies Ltd and requires specific API keys and search engine IDs stored in environment variables.

### Important Classes and Functions:
- **`WebSearcher` Class**: A class to handle web searches using the Google Custom Search Engine API.
- **`__init__` Method**: Initializes the `WebSearcher` class with an output location.
- **`search` Method**: Executes the search based on given query parameters, iterates through multiple pages of results, and populates a list of `PipelineItem` objects with the URLs retrieved from the search results.


**workflow.py**

The `Freezable` class prevents the addition of new attributes to an object once it is "frozen" by setting `_is_frozen` to True.

The `PipelineItem` class, inheriting from `Freezable`, represents a work item with attributes such as `id`, `path`, `summary`, etc., and it prevents adding new attributes. It also includes equality and comparison methods based on the `path` and `summary`.

The `Theme` class, also inheriting from `Freezable`, represents a documented cluster of items with attributes like `short_description` and `long_description`. It includes equality and comparison methods based on these attributes.

`PipelineStep` is a base class that initializes with an `output_location` attribute.

`PipelineSpec`, inheriting from `Freezable`, outlines the workflow with attributes like `clusters`, `description`, and `themes`.

`WebSearchPipelineSpec`, extending `PipelineSpec`, adds attributes like `pages`, `search_key`, and `mail_to`, and is frozen to prevent additions.

`YouTubePipelineSpec`, a `Freezable` class, specifies attributes for downloading video playlists like `playlists` and `max_words`.

`HtmlDirectedPipelineSpec`, also `Freezable`, sets up a list of URLs to download.

`FileDirectedPipelineSpec`, another `Freezable` class, establishes a list of files to download.

`PipelineFileSpec`, inheriting from `Freezable`, sets attributes for the Waterfall workflow, including `output_data_name` and `description`.

**youtube_searcher.py**

This code gathers information about YouTube videos from specified playlists and processes them into `PipelineItem` objects.

- **Libraries Used:** Imports necessary Google API libraries and other essential modules like `logging`, `os`, and `datetime`.
- **Logging Setup:** Configures logging to display warnings and higher severity messages.
- **API Configuration:** Retrieves YouTube API credentials and sets API-specific constants.
- **`parseVideoDurationMins` Function:** Parses video duration from the ISO 8601 format to minutes.

**Important Classes and Functions:**

- **`parseVideoDurationMins(duration: str) -> int`:** Converts video duration from ISO 8601 to minutes.
- **`YoutubePlaylistSearcher`:** 
  - **`__init__(self, output_location: str)`:** Initializes the searcher with an output location.
  - **`search(self, pipeline: YouTubePipelineSpec) -> list[PipelineItem]`:** Searches specified playlists and generates a list of `PipelineItem` objects.

**youtube_transcript_chunker.py**

This script divides the transcript of a YouTube video into manageable chunks.

The `make_start_time_offset` function calculates a time marker in the YouTube video based on the given minutes. It formats the marker to include hours and minutes if necessary.

The `YouTubeTranscriptChunker` class, which inherits from `PipelineStep`, is designed to chunk a transcript using the `Chunker` class. It initializes with an output location and the `chunk` method divides the transcript into chunks with specified word limits and overlaps. If the transcript is short, it handles special cases like having only one chunk or no chunks at all.

`chunk` method also computes chunk timings assuming an even distribution of text.

**youtube_transcript_downloader.py**

This script downloads the YouTube transcript of videos in a playlist. 

The `clean_text` function cleans up new lines, redundant characters, and inaudible markers from the raw transcript. The `parse_video_id` function extracts and returns the video ID from a YouTube URL. 

The `YouTubeTranscriptDownloader` class inherits from `PipelineStep` and downloads and saves the transcript of a given YouTube video. It has methods to check if the transcript already exists in the repository and handles exceptions such as no transcript available, transcripts disabled, and video unavailable. 

Important classes/functions:
1. `clean_text`
2. `parse_video_id`
3. `YouTubeTranscriptDownloader`
****************************************

****************************************
Waterfall\test\ReadMe.Salon.md
****************************************
**test_boxer_pipeline.py**

This script is a test module for the Boxer Data Pipeline implementation provided by Braid Technologies. It imports necessary libraries and specifies paths for the test environment. Logging is configured to show detailed information about script execution.

Three test functions are defined:

1. `test_youtube_boxer_pipeline()`: This function tests the Boxer Pipeline for YouTube content. It creates a short playlist from the first YouTube entry and asserts whether the pipeline has successfully fetched items.

2. `test_html_boxer_pipeline()`: This function tests the Boxer Pipeline for HTML content. It uses data from HTML pages and verifies if the pipeline can retrieve the items successfully.

3. `test_full_boxer_pipeline()`: This function is intended for a full pipeline test with both YouTube and HTML content combined. It is currently set to return immediately for quick testing purposes.

Important classes and functions:
- `BoxerDataPipeline`
- `YouTubePipelineSpec`
- `HtmlDirectedPipelineSpec`
- `PipelineFileSpec`
- `test_youtube_boxer_pipeline()`
- `test_html_boxer_pipeline()`
- `test_full_boxer_pipeline()`

**test_chunker.py**

This code sets up a test environment and defines multiple pytest functions to test a text Chunker and an HTML file downloader.

It begins by importing necessary modules and configuring logging. It proceeds by altering the system path to include the parent and source directories to ensure that the modules are importable.

The code defines a pytest fixture, `test_output_dir`, which creates and later cleans a temporary directory for test outputs.

Five test functions are then defined: `test_basic`, `test_with_output`, `test_long`, `test_long_with_overlap`, and `test_long_overlap`. These tests mainly check the functionality of `Chunker` and `HtmlFileDownloader` classes and the `PipelineItem` class by asserting proper output or behaviors, such as chunking of text.

**test_cluster_analyser.py**

This Python code is a test suite using the `pytest` framework. It begins by setting up necessary imports and path configurations to ensure the script can access the modules in the 'src' directory.

A logging configuration is established to capture and display log messages, with a specific logger set to the ERROR level.

The script employs `pytest` fixtures to create and clean up a temporary directory for storing test outputs. 

Two test functions are defined: `test_basic` and `test_with_output`. 

- `test_basic` creates a `PipelineItem` and verifies that the `ClusterAnalyser` is configured with the correct output location.
- `test_with_output` simulates a pipeline where `PipelineItem` objects are downloaded, summarised, embedded, and then analysed using `ClusterAnalyser`.

Important classes and functions:
- `PipelineItem`
- `ClusterAnalyser`
- `HtmlFileDownloader`
- `Summariser`
- `Embedder`
- `test_output_dir` (fixture)
- `test_basic`
- `test_with_output`

**test_db_repository.py**

This code consists of tests for the DB API utilizing the Python `unittest` framework.

The `test_basic` function verifies the basic construction of a `DbRepository` object with an `application_id` and `test_context`.

The `test_does_not_exist` function checks if a specific path does not exist in the repository.

The `test_save` function tests the saving functionality of an item in the repository, asserting that the save operation returns `true`.

The `test_save_exists` function saves an item and verifies its existence afterward.

The `test_save_load` function saves an item and then checks if the item can be correctly loaded, confirming that the saved and loaded data match.

Important Classes/Functions:
- `test_basic()`
- `test_does_not_exist()`
- `test_save()`
- `test_save_exists()`
- `test_save_load()`
- `DbRepository`
- `PipelineItem`

**test_embedder.py**

This code is a test module using pytest, focusing on integrating and testing components of a workflow.

The `test_output_dir` fixture creates a temporary directory for test outputs, ensuring a clean environment for each test. It logs the creation and deletion of this directory.

The `test_basic` function initializes an `Embedder` with the temporary output directory and verifies that the embedder's output location is set correctly.

The `test_with_output` function tests the integration of `PipelineItem`, `HtmlFileDownloader`, and `Embedder`. It changes the working directory, sets up a pipeline item with a test HTML path, downloads the HTML file, processes the text, and embeds it, asserting that the output is not empty.

Important classes and functions include `test_output_dir`, `test_basic`, `test_with_output`, `PipelineItem`, `Embedder`, and `HtmlFileDownloader`.

**test_embedding_finder.py**

The code imports several standard libraries such as `pytest`, `os`, `shutil`, `sys`, and `logging`. 

It configures the logging utility to display messages at the WARNING level by default and sets the logger to ERROR level for the current module.

The code extends the system path to include the parent directory and the `src` directory for importing custom modules.

It defines a pytest fixture `test_output_dir` to create and clean up a temporary directory for storing test outputs. 

The `test_basic` function tests an instance of `EmbeddingFinder` using a proxy embedding array.

The `test_with_output` function tests downloading, summarising, and embedding HTML files, and then uses `EmbeddingFinder` to find the nearest embedding to a given text.

Important classes and functions include `PipelineItem`, `EmbeddingFinder`, `HtmlFileDownloader`, `Summariser`, `Embedder`, `test_output_dir`, `test_basic`, and `test_with_output`.

**test_embedding_repository.py**

This script uses the pytest framework to test functionalities of an `EmbeddingRespositoryFacade` class from the `src.embedder_repository_facade` module.

It sets up logging configuration to capture and print log messages. The log level is set to ERROR to minimize log output during test runs.

A pytest fixture `test_output_dir` creates a temporary directory for test output and ensures cleanup after the tests.

Three test functions are provided:
1. `test_basic` ensures that the `EmbeddingRespositoryFacade` instance correctly sets the output location.
2. `test_with_output` tests saving and loading functionality, ensuring data is correctly saved and retrievable.
3. `test_with_no_output` tests loading a non-existent file, ensuring the process catches exceptions as expected.

**test_errors.py**

**Classes and Functions:**

- `PipelineItem`
- `Summariser`
- `test_basic`
- `test_with_output`

Firstly, the script modifies the system path to ensure it can import modules from the parent and source directories. It sets up logging configuration to capture error messages with a specific format.

The `test_basic` function creates log entries with different severity levels (error, warning, info, and debug) and asserts `True`. 

The `test_with_output` function changes the current working directory to the test's root directory and defines the input and output locations. It then initializes a `PipelineItem` instance, assigns text to it, and uses a `Summariser` instance to process the text. Finally, it asserts that the summary generated by the `Summariser` is not empty.

**test_file_repository.py**

This code sets up tests for a File System API, specifically targeting the `FileRepository` class located in `src/file_repository.py`.

Logging is configured to record warnings and higher severity messages, with the logger set to capture error level logs.

There are three primary test functions defined using `pytest`: 
- `test_basic` validates that the repository's output location is correctly set.
- `test_with_output` checks if a file can be saved, its existence verified, and its content correctly retrieved.
- `test_with_no_output` verifies that non-existent files are correctly identified and don't erroneously retrieve expected content.

A fixture, `test_output_dir`, creates a temporary directory for test output and ensures cleanup after tests.

**test_html_file_downloader.py**

This module sets up a testing environment for a web-scraping application using the `pytest` framework. It configures the environment to include necessary directories in the system path for module imports.

Logging configurations are set up to assist in monitoring the script execution and capturing potential errors.

A `test_output_dir` fixture is created to generate a temporary directory for storing test results. This fixture also ensures cleanup after test completion.

The module tests the functionality of `HtmlFileDownloader` from `src.html_file_downloader`, ensuring it correctly handles downloading HTML files and processing them through `PipelineItem` from `src.workflow`.

Important functions and classes:
- `test_output_dir`: A pytest fixture for managing temporary directories.
- `test_basic`: Tests basic functionality of `HtmlFileDownloader`.
- `test_with_output`: Tests downloading a local HTML file.
- `test_connected`: Tests downloading an HTML file from a URL.

**test_html_link_crawler.py**

This code sets up a testing environment using pytest for a web-crawling application.

It configures logging to capture errors and declares important directories to be included in the system path. The main classes involved are `PipelineItem` and `HtmlLinkCrawler` from `src.workflow` and `src.html_link_crawler` respectively.

A fixture, `test_output_dir`, is defined to create and clean up a temporary directory for test outputs. Several test functions are defined (e.g., `test_basic`, `test_with_output`, `test_with_one_recursion`, etc.) to validate various functionalities of `HtmlLinkCrawler`, such as handling different recursion depths and crawling specific web pages. Each test asserts the correctness of the crawlers behavior.

**test_summariser.py**

This Python script is a test module for `PipelineItem`, `Summariser`, and `HtmlFileDownloader` classes. It uses `pytest` for testing and includes setup for a temporary test output directory.

Logging is configured to display warnings and errors in a specified format.

A fixture function `test_output_dir` is defined to create and clean up a temporary directory for test outputs.

The `test_basic` function verifies that a `Summariser` instance correctly sets its output location.

The `test_with_output` function tests the process of downloading an HTML file using the `HtmlFileDownloader` and then summarizing it with the `Summariser`.

Key Classes/Functions: `PipelineItem`, `Summariser`, `HtmlFileDownloader`, `test_output_dir`, `test_basic`, `test_with_output`.

**test_summarise_fail_suppressor.py**

- The code sets up a testing environment using the pytest framework for a Python project.
- The `test_output_dir` fixture creates a temporary directory for test outputs and ensures its cleanup after the test.
- Logging is configured to display warnings and errors, aiding in debugging.
- Path adjustments are done to include parent and source directories in the system paths.
- The code imports necessary classes including `PipelineItem` from `workflow`, `SummariseFailSuppressor` from `summarise_fail_suppressor`, and `HtmlFileDownloader`.
- `test_basic`, `test_with_no_suppression`, and `test_with_suppression` are defined to validate `SummariseFailSuppressor` functionality.
- These tests check the summariser's output location, its behavior with valid summaries, and scenarios where suppression is applied.

**test_summary_repository.py**

This code is a set of tests for the `SummaryRepositoryFacade` class from the `summary_repository_facade` module, using the `pytest` framework. 

The code imports necessary libraries, sets up paths for module imports, and configures logging.

A `pytest` fixture named `test_output_dir` is defined to create and clean up a temporary directory for test outputs.

The `test_basic` function tests the creation of a `SummaryRepositoryFacade` object and verifies its output location.

The `test_with_output` function tests saving, checking existence, and loading of a file, ensuring the file's content is correctly saved and retrieved.

The `test_with_no_output` function tests that a non-existent file is correctly reported as not existing and not containing expected text.

**test_text_repository.py**

This script sets up tests for the `TextRepositoryFacade` API, which involves operations like saving, loading, and checking the existence of text files in a repository.

A logging system is configured to track the execution at the ERROR level.

A pytest fixture, `test_output_dir`, creates and then cleans up a temporary directory used for test outputs.

Three test functions (`test_basic`, `test_with_output`, and `test_with_no_output`) verify different functionalities of the `TextRepositoryFacade` class:
- `test_basic` checks the initialization of the repository.
- `test_with_output` verifies saving and loading text with successful file existence.
- `test_with_no_output` validates behavior with non-existent files.

Key Component:
- `TextRepositoryFacade`: Primary class for managing text file operations in the repository.

**test_theme_finder.py**

This code sets up the environment for testing by configuring the script's path and logging settings.

The logging is configured to display messages with a severity level of ERROR, while the log format includes timestamps, logger name, log level, and message content.

The important classes imported from the 'src' module are `PipelineItem`, `ThemeFinder`, `Summariser`, and `HtmlFileDownloader`.

Two test functions are defined: `test_basic()` and `test_with_output()`. `test_basic()` creates an instance of `ThemeFinder` and ensures it is not None. `test_with_output()` processes a list of HTML files, downloading them using `HtmlFileDownloader`, summarising them with `Summariser`, and then finding the theme using `ThemeFinder`. The final theme is asserted to be non-empty.

**test_waterfall_pipeline.py**

This module consists of test cases for the `WaterfallDataPipeline` system. The tests employ the `pytest` framework to verify various functionalities of the pipeline:

- `test_basic`: Checks if a `WaterfallDataPipeline` object is correctly initialized with the specified output location.

- `test_with_search_supply`, `test_with_search_demand`, `test_with_search_telecom`, `test_with_search_nationwide`, and `test_with_search_bny`: Each test configures a `WebSearchPipelineSpec` object with specific search keys and other parameters, running dynamic searches for different datasets while ensuring at least one link is returned.

- `test_with_search_vf_survey_01`: Tests a static search using predefined file lists with `PipelineSpec` and `FileDirectedPipelineSpec` to ensure the links returned are as expected.

Logging is configured to aid in debugging and monitoring the execution. Key classes and functions include `WaterfallDataPipeline`, `WebSearchPipelineSpec`, `PipelineSpec`, and `FileDirectedPipelineSpec`.

**test_web_searcher.py**

This script sets up basic configurations for testing a web search functionality.

It modifies the system path to include the parent and 'src' directories so that modules can be imported easily.

Logging is configured to display warning and error messages.

There are two test functions: 
- `test_basic()` initializes a `WebSearcher` object with a specified output location and checks if the output location is set correctly.
- `test_with_search()` changes the working directory to the test root, initializes a `WebSearcher` and a `WebSearchPipelineSpec` object, sets up the search configurations, and verifies that at least one item is returned in the search results.

Important classes/functions:
- `WebSearcher`
- `WebSearchPipelineSpec`
- `test_basic`
- `test_with_search`

**test_workflow.py**

This code tests components of a web search pipeline. 

**Important Classes/Functions:**
1. **PipelineItem**: Initializes an object with a URL, summary, and an embedding vector. Contains tests for its property assignments and TypeError handling.
   
2. **Theme**: Creates a theme with descriptions and example pipeline items. Includes tests for item assignments and verifying proper error handling for incorrect properties.

3. **WebSearchPipelineSpec**: Sets up a web search pipeline with a search key, description, themes, and an output chart name. Verifies the proper configuration and error handling.

Additionally, it configures logging to display errors and modifies the system path to include the source directory for module imports.

**test_youtube_playlist.py**

The script imports various modules, setting up paths and configurations essential for the subsequent code execution.

Logging is configured to warn of any issues during script execution, with errors being prominently logged to facilitate debugging.

The `YouTubePipelineSpec`, `YoutubePlaylistSearcher`, and `YouTubeTranscriptDownloader` classes are imported from their respective modules in the `src` directory.

It includes three test functions: `test_basic()` verifies the proper initialization of the `YoutubePlaylistSearcher`, `test_with_search()` ensures that searches yield results using predefined YouTube playlists, and `test_download()` checks the functionality of downloading transcripts from search results and ensures they contain text.
****************************************

****************************************
WaterfallBrowser\src\ReadMe.Salon.md
****************************************
**App.tsx**

This React module renders an application using Fluent UI components. The `App` function is the main component that handles the retrieval of chunk data based on a URL parameter.

The `fluidFillPageStyles`, `pageOuterStyles`, and `innerColumnStyles` provide CSS-in-JS styles for various layout configurations. These styles are applied respectively to the root, outer container, and inner column elements to ensure a responsive and flexible layout.

`ChunkRetriever` is a component that receives a `chunkId` parameter from the URL and a retrieval function `retrieveChunk` to fetch relevant data.

The application is initialized and rendered to the DOM within an element with the ID `reactRoot`. For performance measurement, it uses `reportWebVitals`.

Important functions and classes:
- `App`
- `fluidFillPageStyles`
- `pageOuterStyles`
- `innerColumnStyles`
- `ChunkRetriever`

**ChunkRetriever.tsx**

The module defines a React component `ChunkRetriever` that fetches and displays a data chunk based on a `chunkId`. It uses hooks to manage fetching state (`chunk`, `calling`, `called`). If the chunk is successfully retrieved using `props.retrieverFn`, it displays the chunk using `ChunkView`; if loading, shows `ChunkViewLoading`; otherwise, shows `ChunkViewError`.

Key functions and classes:
- `ChunkRetriever`: Main React component to retrieve and display data chunk.
- `ChunkRetrieveFunction`: Type defining a function signature for chunk retrieval.
- `retrieveChunk`: Asynchronously retrieves a data chunk using `ChunkRepostoryApi` and a decoded API key.

**ChunkView.tsx**

- The code defines several functions and a `ChunkView` component in a React module.
- `chunkUrl(value: string)`: Constructs a URL string with `value` as a query parameter.
- `backToParent(value: string | undefined)`: Generates a `ReactNode` with a link to the parent element if `value` is provided.
- `mapRelated(value: string, index: number, all: Array<string>)`: Maps string values to `ReactNode` elements containing hyperlinks.
- `splitByNewlines(text: string)`: Splits the input text into an array of non-empty strings based on newline characters.
- `ChunkView(props: {chunk: IStoredChunk})`: A React component that displays a stored chunk's title, summary, and URL. It also includes navigation to the parent chunk and related chunks.


**ChunkViewError.tsx**

The `ChunkViewError` component is a React functional component. 

It imports React and two string values, `uiAppName` and `uiSorryNoData`, from a module named `UIString`.

Within the component, it returns a JSX structure consisting of a `div` element. Inside the `div`, it renders a paragraph containing `uiAppName` inside bold tags, another paragraph displaying `uiSorryNoData`, and some non-breaking spaces (&nbsp;) for spacing.

The component is then exported as the default export from the module. 

Important functions/classes:
- `ChunkViewError`: React functional component.
- `uiAppName` and `uiSorryNoData`: Imported string values.

**ChunkViewLoading.tsx**

This code imports React and two variables, `uiAppName` and `uiLoading`, from a module named `UIString`.

The function `ChunkViewLoading` is defined, which is a React functional component. It returns a JSX fragment containing a `div` element with two `p` tags.

The first `p` tag displays the `uiAppName` variable in bold.

The second `p` tag displays the `uiLoading` variable.

The `ChunkViewLoading` function is exported as the default export from this module.

**Defusc.tsx**

This code defines a function `getDefusc` that decodes an obfuscated string. 

The function starts by initializing a string variable `obfusc` with an obfuscated value, which is encoded in Base64 format.

It then uses the `atob` function to decode this Base64-encoded string, storing the resulting decoded string in the variable `defusc`.

Finally, the function returns the decoded string stored in `defusc`.

The primary function in this module is `getDefusc`.

**reportWebVitals.ts**

The `reportWebVitals` function conditionally imports and reports web vitals data. 

It accepts an optional `onPerfEntry` callback function. If `onPerfEntry` is provided and is a function, it dynamically imports specific functions from the 'web-vitals' module.

The imported functions (`getCLS`, `getFID`, `getFCP`, `getLCP`, `getTTFB`) are then executed with `onPerfEntry` as their parameter to measure various web vital metrics.

The function is exported as the default export of the module. 

Important function: `reportWebVitals`.

**UIString.ts**

This code defines a module that exports several string variables used for UI messaging in an application named "Waterfall Browser".

- `uiAppName` contains the name of the application.
- `uiSorryNoData` provides a message for situations where data with a specific ID cannot be found.
- `uiBackToParentChunk` holds the text for a navigation option to return to a parent chunk of data.
- `uiRelatedChunks` is used for labeling related data chunks.
- `uiLoading` serves as a loading message.

Important elements include each exported variable which can be used for internationalization or to maintain consistent text across various UI components.
****************************************

****************************************
WaterfallBrowser\test\ReadMe.Salon.md
****************************************
**chunkretriever.test.tsx**

This code is a test suite for the `ChunkRetriever` component, using Mocha as the test framework, `expect` for assertions, and `@testing-library/react` for rendering the component and querying the DOM.

The `waitFor` function is defined to pause execution for a specified number of seconds.

Within the `describe` block, a test case named "Renders sample Chunk" is provided as an `it` function. This test asynchronously renders the `ChunkRetriever` component with specific props, waits for 19 seconds, and then checks for the presence of elements containing the text "Summary", "Title", and "microsoft".

The `.timeout(20000)` method sets a timeout of 20 seconds for both the `describe` and `it` blocks to ensure tests have ample time to complete.

Important functions and classes:
- `describe`
- `it`
- `waitFor`
- `render`
- `screen.getByText`
- `ChunkRetriever`

**ChunkTestHelpers.tsx**

The code defines and exports several variables representing chunks of data, adhering to the `IStoredChunk` interface from an imported module.

It initializes `parentKey`, `childKey1`, `childKey2`, and `modelKey` with specific string values and also sets the current date as `now`.

Three chunks`testChunk`, `childChunk1`, and `childChunk2`are created as objects implementing the `IStoredChunk` interface. `testChunk` is designated as `parentChunk` for reference.

A function `testChunkRetriever` is defined to retrieve a chunk based on its ID, returning the corresponding chunk or `undefined` if the ID doesn't match known keys.

**Important classes/functions:**
- `IStoredChunk`
- `testChunkRetriever`

**chunkview.test.tsx**

This code defines automated tests for a `ChunkView` React component using Mocha and Expect libraries.

First, it imports necessary modules and the `ChunkView` component along with the `testChunk` data. 

The `describe` function sets up a test suite named "ChunkView". 

Within this suite, the `it` function describes a test case "Renders sample Chunk". 

Inside the test case, `render` is used to render the `ChunkView` component with the provided `testChunk` data. 

The `screen.getByText` method checks if certain text elements ("Summary", "Title", "microsoft") are present in the rendered component.

`expect` assertions confirm that these elements are correctly rendered.
****************************************

****************************************
Api\src\functions\ReadMe.Salon.md
****************************************
**AzureStorableApi.ts**

The `AzureStorableApi` module provides HTTP endpoints to perform CRUD operations on `IStorable` objects stored in Azure Cosmos DB via Azure Functions. 

Classes/Functions:
- `applyTransformer` and `applyArrayTransformer`: Apply a transformer function to storable objects or arrays.
- `findStorableApi`, `getStorableApi`, `getStorableApiFromQuery`, `getStorableApiCommon`: Retrieve storable objects from Azure Cosmos DB with optional data transformation and session validation.
- `saveStorableApi`: Validate session keys and save storable objects, logging the operation status.
- `removeStorableApi`: Validate session keys and remove storable objects, logging the operation status.
- `getRecentStorablesApi`: Retrieve recent storable objects with an option to transform data and log the operation.

**CheckSession.ts**

**Important Functions and Modules:**
1. `checkSession(request: HttpRequest, context: InvocationContext)`: This function validates the session key from the query parameters of the request. It checks if the session key matches the expected values stored in environment variables. Based on the validity, it returns a 200 status with the session key or a 401 status with an authorization failure message.

2. `app.http('CheckSession', { methods: ['GET', 'POST'], authLevel: 'anonymous', handler: checkSession })`: This code configures an HTTP endpoint named "CheckSession" that supports GET and POST methods, sets anonymous access, and handles requests using the `checkSession` function.

**Summary:**
- This Azure Function module validates session keys against environment-configured valid sessions.
- It supports GET and POST methods.
- The `checkSession` function handles session validation, returning different responses based on session validity.
- The function is publicly accessible but requires a valid session key for successful authentication.

**Chunk.ts**

### Functionality
The code defines an Azure Function module for text chunking, which splits input text into manageable chunks with configurable size and overlap settings.

### Important Classes/Functions
- **chunkText**: Utilizes the `model.chunkText` method to split input text based on specified chunk size and overlap words.
- **chunk**: Asynchronous function that handles HTTP requests, validates session, and returns text chunks or error messages.

### Key Points
- The module is part of Braid API infrastructure and ensures authenticated sessions.
- The `chunk` function parses the HTTP request, validates the session, and processes the input text if valid.
- In case of errors, it returns standardized error responses.
- The Azure Function configuration is specified for anonymous access and supports GET and POST methods.

**Classify.ts**

This module, **Classify**, is an Azure Function that provides text classification services using the OpenAI API. 

The **`decodeClassification`** function converts initial classification results into human-readable formats.

The **`singleShotClassify`** function classifies input text into predefined categories using OpenAI's model. It retries up to five times on network errors or rate limit hits.

The **`classify`** function handles text classification requests. It validates the session, reads the request JSON, and uses the `singleShotClassify` function to classify the text. It then returns the classification result or an appropriate error message.

The main module imports necessary functions and sets up the Azure HTTP handler with authentication level set to 'anonymous'.

**CosmosRepositoryApi.ts**

The `CosmosRepositoryApi` module provides utility functions for interacting with Azure Cosmos DB. It generates authorization tokens and headers compliant with Microsoft's authentication requirements.

**Key Functions:**

1. `getAuthorizationTokenUsingMasterKey`: Generates authorization tokens using an HTTP request's verb, resource type, resource ID, date, and master key.
2. `storableToken`: Creates tokens for various storage operations using the master key.
3. `makeStorableDeleteToken`: Generates tokens for delete operations.
4. `makeStorablePostToken`: Generates tokens for post operations.
5. `makePostHeader`: Creates headers for POST requests to Azure Cosmos DB.
6. `makeDeleteHeader`: Creates headers for delete requests.
7. `makePostQueryHeader`: Generates headers for query operations, supporting continuation tokens.

These functions utilize the `crypto` module to generate necessary signatures and authorization tokens for secure communication with Azure Cosmos DB's REST API.

**CosmosStorableApi.ts**

The code provides a high-level API for interacting with Azure Cosmos DB storage, implementing CRUD operations and query capabilities for storable objects.

**Important Classes/Functions:**
1. **Interfaces and Constants:**
   - `ICosmosStorableParams` and `ILoggingContext`: Define parameters and logging methods.
   - Constants for partition keys and collection paths like `chunkPartitionKey`, `chunkCollectionPath`.

2. **Logging Classes:**
   - `AzureLogger` and `ConsoleLogger`: Implement different logging functionalities using Azure Functions or Console.

3. **Utility Functions:**
   - `applyTransformer`: Applies an optional transformation to storable objects.

4. **Asynchronous CRUD Functions:**
   - `findStorable`, `loadStorable`, `saveStorable`, `removeStorable`: Perform database operations (find, load, save, and remove).

5. **Query Functions:**
   - `loadRecentStorables`, `loadStorables`: Load multiple storable objects from the database based on query specifications.

Overall, the module highlights usage of Azure Cosmos DB for data storage, ensuring efficient query execution while maintaining robust logging capabilities.

**Embed.ts**

The code is an Azure Function for embedding text via the Azure AI service.

**Key Functions and Classes:**
1. **calculateEmbedding**:
   - Asynchronously calculates the embedding of given text.
   - Uses `axios` with retries managed by `axiosRetry` in case of rate limits (HTTP 429) or network errors.
   - Sends a POST request to the Azure AI service and extracts the embedding from the response.

2. **embed**:
   - Handles HTTP requests to embed text data.
   - Validates session using `isSessionValid`.
   - Extracts and processes the text from the request.
   - Summarizes text if it exceeds context window size using `recursiveSummarize`.
   - Obtains embeddings via `calculateEmbedding`.
   - Returns the embedding in the HTTP response or appropriate error responses.

3. **app.http**:
   - Defines HTTP endpoint 'Embed' for GET and POST methods with anonymous authentication, handling requests using the `embed` function.

**Utilities and Helper Imports:**
- `isSessionValid`, `sessionFailResponse`, `defaultErrorResponse`, `invalidRequestResponse`: Utility functions for session validation and error handling.
- `getDefaultModel`: Fetches default model settings.
- `recursiveSummarize`: Summarizes text when necessary.

**Summary:**
This module defines an Azure Function `embed` that processes incoming HTTP requests, validates the session, optionally summarizes the input text, and calculates an embedding using a remote AI service, returning the result or appropriate errors based on the flow and validations.

**EnrichedChunkRepository.ts**

The code provides a module for handling and querying enriched chunks of data from memory using various criteria.

**Important functions and classes:**

1. `cosineSimilarity`: Computes the cosine similarity between two vectors, used for assessing relevance.
2. `lookLikeSameSource`: Compares two URLs to determine if they are from the same source, handling specific rules for YouTube and GitHub URLs.
3. `lowestOfCurrent`: Finds the index of the least relevant chunk in an array, optionally considering URL similarities.
4. `replaceIfBeatsCurrent`: Decides whether a candidate chunk should replace an existing chunk in an array based on relevance.
5. `EnrichedChunkRepositoryInMemory`: The main class storing chunks and providing methods to query chunks relevant to a summary or URL.

**Methods in EnrichedChunkRepositoryInMemory:**
1. `lookupRelevantFromSummary`: Finds relevant chunks by comparing a summary.
2. `lookupRelevantFromUrl`: Finds relevant chunks by comparing embeddings from a URL.
3. `lookupFromUrl`: Retrieves detailed information about a chunk matching a specific URL.

The code also leverages assert functions, calculates embeddings, and sorts results based on relevance.

**EnrichedChunkRepositoryDb.ts**

The `EnrichedChunkRepositoryDb` class implements the `IEnrichedChunkRepository` interface and manages enriched chunks both in an in-memory repository and through asynchronous data loading from a database.

The class constructor initializes the in-memory repository by loading chunk data from the database and setting up a semaphore to handle asynchronous operations.

Key methods include:
- `lookupRelevantFromSummary`: Searches for enriched chunks relevant to the summary in the query.
- `lookupRelevantFromUrl`: Searches for relevant enriched chunks based on a given URL.
- `lookupFromUrl`: Retrieves the summary of an enriched chunk by its URL.

Important classes and functions in the module:
- `EnrichedChunkRepositoryDb`
- `lookupRelevantFromSummary`
- `lookupRelevantFromUrl`
- `lookupFromUrl`

**EnrichedChunkRepositoryFactory.ts**

The code defines a function called `getEnrichedChunkRepository` which returns an instance of `IEnrichedChunkRepository` based on the provided `repository` type, specified by the `EChunkRepository` enumeration.

The function implements a singleton pattern, ensuring that only one instance of `EnrichedChunkRepositoryDb` is created and reused. This is important because repository instances are relatively expensive to create.

The function checks the repository type and creates an instance of `EnrichedChunkRepositoryDb` if it hasnt been created already, and returns this instance. Currently, it handles `kWaterfall` and `kBoxer` repository types in the same way, but the structure allows for future extension.

**EnumerateModels.ts**

This code defines an Azure Function that handles HTTP requests and returns details of installed models. 

It imports required modules from `@azure/functions`, custom utilities, and types from a common TypeScript source.

A model is retrieved using `getDefaultModel()`.

The `enumerateModels` function is the key function utilized here. It checks if the session is valid with `isSessionValid(request, context)`. If valid, it processes the HTTP request, logs the details, and returns model specifications in JSON format. If any error occurs, it logs the error and sends a default error response.

The `app.http` method registers the `enumerateModels` function for handling 'GET' and 'POST' requests anonymously.

Important functions: `enumerateModels`.

Key modules: `@azure/functions`, `getDefaultModel`, `IEnumerateModelsRequest`, `IEnumerateModelsResponse`, `sessionFailResponse`, `defaultErrorResponse`, `isSessionValid`.

**EnumerateRepositories.ts**

The code defines an Azure Function named `enumerateRepositories` within a Node.js module.

The `enumerateRepositories` function is an asynchronous HTTP-triggered function that sends back details of installed repositories. It accepts `HttpRequest` and `InvocationContext` objects as parameters and returns an `HttpResponseInit` object.

The function checks if the session is valid using the `isSessionValid` utility function. If valid, it processes the JSON request and returns a response containing the repository details. If an error occurs, it logs the error and returns the default error response. If the session is invalid, a session failure response is returned.

The function is registered with the Azure Function app, supporting both GET and POST methods and having anonymous authentication level. 

Important functions: `enumerateRepositories`.

**FindEnrichedChunks.ts**

This module contains functions for an Azure Function App to search for enriched chunks of data based on query specifications.

**Key Functions:**
1. **FindRelevantEnrichedChunksFromSummary:** Validates the session and retrieves chunks relevant to the summary specification provided in the request.
2. **FindRelevantEnrichedChunksFromUrl:** Validates the session and retrieves chunks relevant to the URL specification in the request.
3. **FindEnrichedChunkFromUrl:** Validates the session and retrieves single chunks from the URL specification in the request.

**Utility Functions:** 
1. **isSessionValid, sessionFailResponse, defaultErrorResponse**: Handle session validation and response generation.
2. **getEnrichedChunkRepository**: Retrieves the repository for data operations.

HTTP endpoints for these functions are registered using `app.http()`.

**FindTheme.ts**

### Important Classes/Functions
- `findThemeCall`
- `findTheme`
- `app.http`

### Code Summary
The code defines an Azure Function to find a common theme in given text. The `findThemeCall` function asynchronously makes a POST request to an Azure endpoint using `axios` to retrieve the theme, with `axiosRetry` providing up to 5 retries for rate limit errors.

The `findTheme` function processes HTTP requests, validates the session key, extracts text and length from the request, ensures the text meets the minimum length, and calls `findThemeCall` to get the theme. The result is returned in an HTTP response, with appropriate error handling for validation, exceptions, and invalid requests.

**GenerateFluidToken.ts**

This code defines an Azure Function for generating Fluid tokens.

The initial setup imports necessary modules from Azure Functions and custom utility functions that check session validity and handle responses. It also imports the `generateToken` function from the Fluid Framework server services.

The `ScopeType` enum is redefined to include specific access levels for the Fluid container/document.

The main function, `generateFluidToken`, validates the session by calling `isSessionValid`. It checks for the presence of necessary credentials (tenantId and key). If valid, it processes the request, extracts relevant data, generates a Fluid token using `generateToken`, and returns the token.

Finally, the function is configured with HTTP methods (GET, POST) and its handler in the Azure function app. 

Important functions and classes include:
- `generateFluidToken`
- `isSessionValid`
- `sessionFailResponse`
- `defaultErrorResponse`
- `generateToken`
- `ScopeType` enum.

**GenerateQuestion.ts**

This module provides an Azure Function that generates questions based on user inputs by interacting with OpenAI's StudioLarge model.

**Important Classes or Functions:**
1. `askModel(query: IGenerateQuestionQuery)`: Makes an HTTP POST request to OpenAI's API with retries on rate-limiting or network errors. It composes the prompt from the `personaPrompt` and `questionGenerationPrompt` within the `query` and returns the generated question.
2. `generateQuestion(request: HttpRequest, context: InvocationContext)`: Handles HTTP requests to the Azure Function. It validates the session, parses the request, logs the input and output, calls `askModel` to get the question, and returns the result. If there's an error, it logs the error and returns a default error response.
3. `isSessionValid`, `sessionFailResponse`, and `defaultErrorResponse` (imported from `./Utility`): Utility functions for session validation and error handling.

The Azure Function can be published using 'func azure functionapp publish Braid-Api' or run locally with 'npm start'.

**IEnrichedChunkRepository.ts**

This code defines constants and interfaces for managing enriched content chunks.

`kDefaultSearchChunkCount` is a constant specifying the default number of chunks to search, set to 2. `kDefaultMinimumCosineSimilarity` specifies the minimum cosine similarity threshold for content comparison, set to 0.5.

The `IEnrichedChunkRepository` interface outlines three main methods: 
1. `lookupRelevantFromSummary`, which retrieves an array of relevant enriched chunks based on summary specifications.
2. `lookupRelevantFromUrl`, which retrieves relevant enriched chunks from other sources using URL specifications.
3. `lookupFromUrl`, which fetches an entire chunk summary given a URL.

Important classes or functions in the module:
- IEnrichedChunkRepository
- lookupRelevantFromSummary
- lookupRelevantFromUrl
- lookupFromUrl

**IPromptPersonaFactory.ts**

This code defines and exports a function `getSummariser` that returns a specific summariser persona configuration based on the input parameters.

Three summariser personas are defined using the `IPromptPersona` interface:
- `ArticleSummariserPersona`
- `CodeSummariserPersona`
- `SurveySummariserPersona`

Each persona has `name`, `systemPrompt`, and `itemPrompt` properties.

The `getSummariser` function receives three parameters: `persona`, `wordTarget`, and `textToSummarise`. It returns an IPromptPersona object with customized `systemPrompt` and `itemPrompt` based on `persona` type:
- `kSurveySummariser` for survey summaries.
- `kCodeSummariser` for code summaries.
- `kArticleSummariser` for general text summaries.

**LoginWithLinkedIn.ts**

This module is an Azure Function application that allows users to log in using their LinkedIn credentials.

The main function, `LoginWithLinkedIn`, validates a session key from the request parameters and redirects eligible users to LinkedIn for authentication.

The helper function, `redirectToLinkedIn`, constructs the LinkedIn authorization URL with required query parameters and facilitates the redirection.

`processAuthFromLinkedIn` handles the callback from LinkedIn, processing the authorization code, and redirects the user back to the home page with specific session details.

`redirectBackHomeWithFullPath` retrieves the access token and user profile information from LinkedIn and constructs the redirection URL to the application's home page with necessary details.

Key functions include `LoginWithLinkedIn`, `redirectToLinkedIn`, `processAuthFromLinkedIn`, and `redirectBackHomeWithFullPath`.

**QueryModelWithEnrichment.ts**

This module is designed to handle enriched queries via an Azure function app.

The `askModel` function sends an enriched query to an AI model, processes the responses, and retrieves relevant enriched chunks if specific criteria are met. It supports up to five retries for network issues or rate limiting and uses Axios for HTTP requests.

The `queryModelWithEnrichment` function validates the session, processes the HTTP request, logs the query, calls `askModel`, and returns the result as an HTTP response or handles errors appropriately.

Important functions:
1. `askModel(query: IEnrichedQuery)`: Processes and retrieves responses to an enriched query.
2. `queryModelWithEnrichment(request: HttpRequest, context: InvocationContext)`: Handles HTTP requests, validates the session, and returns the enriched query results.

**StorableActivity.ts**

This module defines endpoints for an Azure Functions application to handle activity records.

The `GetActivity`, `SaveActivity`, `RemoveActivity`, and `GetActivities` endpoints are configured using `app.http` allowing them to process HTTP requests.

`getActivity` is an asynchronous function that retrieves activity records and processes the HTTP request using `getStorableApi`.

`saveActivity` validates the session key, processes the JSON request to save activity records using `saveStorableApi`.

`removeActivity` removes the specified activity records using the `removeStorableApi` function.

`getRecentActivities` retrieves recent activity records using `getRecentStorablesApi`.

The important classes and functions are `app`, `HttpRequest`, `HttpResponseInit`, `InvocationContext`, `getActivity`, `saveActivity`, `removeActivity`, and `getRecentActivities`.

**StorableChunk.ts**

The provided module defines several Azure HTTP functions within an `app` object, designed to handle CRUD operations for "Chunk" records.

`getChunk`, `findChunk`, `saveChunk`, `removeChunk`, and `getRecentChunks` are the primary functions in this module, each mapped to their respective HTTP routes using `app.http`.

Each function validates the session key from the request query parameters, processes the JSON request, and interacts with various helper functions (`getStorableApi`, `findStorableApi`, `saveStorableApi`, `removeStorableApi`, and `getRecentStorablesApi`) imported from internal modules for performing database operations.

The handlers return a properly formatted HTTP response with either the operation result or an error message.

**StorablePage.ts**

This code defines an Azure Function app that handles HTTP requests to get or save page data. 

It imports necessary modules and methods from Azure Functions and some custom internal utilities, including `decompressString`, `pageStorableAttributes`, `getStorableApiFromQuery`, `saveStorableApi`, and various types from common modules.

Two transformer functions, `decompressHtml` and `sendHtml`, are provided to process page data: decompressing HTML content and sending it as an HTTP response, respectively.

The `getPage` function fetches a page record according to query parameters, decompresses its HTML, and returns it, while the `savePage` function saves a page record from the request's JSON payload.

Important classes or functions: 
- `decompressHtml`
- `sendHtml`
- `getPage`
- `savePage`

**StudioForTeams.ts**

- The code is a module for an Azure Function App to handle HTTP queries for a Boxer application.
  
- It uses imports from `@azure/functions`, including `app`, `HttpRequest`, `HttpResponseInit`, and `InvocationContext`.

- Key functions and constants are imported from other modules, like `askModel` from `./QueryModelWithEnrichment`, and various types from `StudioApi.Types` and `EnrichedQuery`.

- The `makeIconPath` function constructs a URL to fetch a websites favicon.

- The main function, `boxerQuery`, processes HTTP requests with a question query parameter, and generates a response with enriched data using `askModel`.

- The function formats the response into an array of `IStudioBoxerResponseEnrichment` objects before returning it.

- The function setup is registered with the Azure Function App using `app.http`.

**Summarize.ts**

1. **Essential Imports and Setup:**
   The code imports necessary modules such as `axios`, and `axiosRetry` for HTTP requests, and various Azure Functions utilities. It uses TypeScript and enforces strict mode.

2. **Text Chunking:**
   The `chunkText` function splits input text into smaller chunks suitable for processing, taking into account an overlap between chunks.

3. **Single-shot Summarization:**
   The `singleShotSummarize` function performs asynchronous summarization of a text. It uses a specified persona and makes HTTP requests to an AI service for summarization, with retry logic for handling rate limits.

4. **Recursive Summarization:**
   The `recursiveSummarize` function performs hierarchical, multi-level summarization by breaking long texts into chunks, summarizing each, and potentially summarizing the combined summary again if needed.

5. **HTTP Handler:**
   The `summarize` function is an HTTP handler for processing HTTP requests to summarize text. It validates sessions, processes input, and calls `recursiveSummarize` to return the summarized text or error responses.

6. **Azure Function Binding:**
   The code finally binds the `summarize` function to the HTTP endpoint "Summarize", handling both GET and POST requests with anonymous authentication.

**Important functions and classes include:**
- `chunkText`
- `singleShotSummarize`
- `recursiveSummarize`
- `summarize`


**TestForSummariseFail.ts**

This code defines a serverless application using Azure Functions to test if a summary correctly conveys the main body of a given text.

**Important Classes/Functions:**
1. `testForSummariseFailCall`: Asynchronously sends a POST request to an Azure endpoint to determine if a summary fails or succeeds. It retries the request up to five times if rate limit errors occur.
2. `testForSummariseFail`: Handles HTTP requests, validates sessions, and uses `testForSummariseFailCall` to process the text and determine if the summary is valid.

**Key Points:**
- Uses `axios` for HTTP requests and `axiosRetry` for retrying failed requests.
- Validates session keys using `isSessionValid`.
- Returns specific error responses with `sessionFailResponse` and `defaultErrorResponse`.
- Configures Azure Functions with defined HTTP methods and authentication levels.

**Utility.ts**

This module is designed for use with Azure Functions and provides several utility functions to handle HTTP responses and session validation. 

The `isSessionValid` function checks if a session provided in the request matches one of the valid session keys stored in the environment variables. It returns `true` if the session is valid and logs the validation result using the provided context.

The `sessionFailResponse` function generates an HTTP response with a status code of 401 (Unauthorized) indicating that the session validation failed.

The `defaultOkResponse` function creates an HTTP response with status code 200 (OK) and a body of "Ok".

The `defaultErrorResponse` function produces an HTTP response with status code 500 (Server Error) to indicate unexpected server issues.

The `invalidRequestResponse` function returns a 400 (Bad Request) status with a custom error message.

The `notFoundResponse` function generates a 404 (Not Found) response with an empty body.

Important functions: `isSessionValid`, `sessionFailResponse`, `defaultOkResponse`, `defaultErrorResponse`, `invalidRequestResponse`, `notFoundResponse`.
****************************************

****************************************
Boxer\scripts\common\ReadMe.Salon.md
****************************************
**ApiConfiguration.py**

This module sets up configuration options for connecting to either Azure OpenAI or OpenAI based on the `azure` flag.

It defines the `ApiConfiguration` class, which initializes several key parameters such as `apiKey`, `apiVersion`, `resourceEndpoint`, `azureDeploymentName`, `azureEmbedDeploymentName`, `modelName`, `embedModelName`, `processingThreads`, `openAiRequestTimeout`, `summaryWordCount`, `chunkDurationMins`, `maxTokens`, and `discardIfBelow`.

Environment variables are safely retrieved for `API_KEY` using `os.getenv`, ensuring compatibility across Windows and Unix systems. The default deployment names, model names, and various thresholds for requests and processing are also defined within the class.

**common_functions.py**

### Important Classes or Functions:

1. **ensure_directory_exists(directory)**: 
   - Checks if a specified directory exists. If not, it creates the directory.

2. **get_embedding(text : str, client : AzureOpenAI, config : ApiConfiguration)**: 
   - Cleans up input text by removing newline characters.
   - Uses `client.embeddings.create` to create an embedding of the text using the configuration from `ApiConfiguration`.
   - Returns the embedding.

### Summary:

This code imports necessary modules and configuration settings. It defines a function, `ensure_directory_exists`, to check for or create a directory. It ensures the existence of a specific directory, `data/web`. Another function, `get_embedding`, processes input text to create and return an embedding using the AzureOpenAI client and API configurations.

**Urls.py**

This Python script logs and processes hit counts on various URLs categorized into YouTube videos, GitHub repositories, and web articles.

Key sections include:
- `youTubeUrls`, `gitHubUrls`, `webUrls`: Store URLs and their descriptions.
- `UrlHit` class: Stores URL details and hit counts.
- `countUrlHits(destinationDir, urls, input_filename, output_filename)` function: Reads a JSON file with hit data, counts occurrences of URL paths, and logs results.
- The function reads the `input_filename` from `destinationDir`, processes hits, then outputs the counts into `output_filename`.
- Error handling and logging are integral throughout for robustness.
****************************************

****************************************
Boxer\scripts\github\ReadMe.Salon.md
****************************************
**download_markdown.py**

- **Imports**: The script imports standard libraries like `os`, `json`, `logging`, `time`, `threading`, `queue`, and `pathlib`, and third-party packages like `markdown` and `BeautifulSoup`.

- **Counter Class**: Defines a thread-safe counter for tracking processed files, which uses a threading lock to ensure synchronization.

- **makeSourceId Function**: Constructs a unique source ID for each file based on the repository directory, repository name, and file path.

- **md_to_plain_text Function**: Converts Markdown content to plain text using `markdown` and `BeautifulSoup`.

- **get_markdown Function**: Reads the Markdown content of a file, converts it to plain text, and saves it as a `.json` file along with metadata. It also skips files that already exist.

- **process_queue Function**: Processes a queue of files, increments the counter, and calls the `get_markdown` function for each file.

- **download_markdown Function**: Main function that sets up the logging, initializes the queue, searches for `.md` files recursively, and processes these files with multiple threads.
****************************************

****************************************
Boxer\scripts\test\ReadMe.Salon.md
****************************************
**test_3chunks_web_pipeline.py**

The module imports necessary libraries and sets up the testing environment by adding the parent directory to the Python path. 

It defines constants for chunking texts and creates configuration and metadata mocks. 

The `calculate_exact_text_length` function calculates the exact number of characters needed for generating chunks, while the `generate_mock_mdd_content` function creates mock content in JSON to simulate an MDD file.

The `create_tokenizer_mock` function mocks the tokenizer for realistic token encoding. 

Test functions like `test_download_html`, `test_append_text_to_previous_chunk`, `test_add_new_chunk`, `test_parse_json_mdd_transcript`, and `test_enrich_text_chunks` validate the functions `download_html`, `append_text_to_previous_chunk`, `add_new_chunk`, `parse_json_mdd_transcript`, and `enrich_text_chunks`, respectively. 

The `mock_file_system` fixture creates a mock file system structure for testing.

**test_utility.py**

This script processes a list of questions through an AI-powered test pipeline.

The main class, `test_result`, stores information about each question, such as the original question, an enriched version, a relevance score, and a follow-up question.

Functions include `get_enriched_question`, `get_text_embedding`, `get_followup_question`, and `assess_followup_question`, all using OpenAI's Azure API to generate and evaluate enriched questions and follow-ups.

The `cosine_similarity` function calculates the similarity between embeddings.
The `run_tests` function orchestrates the entire process: loading data, generating embeddings, comparing them, and logging results.

Important classes and functions:
1. `test_result`
2. `get_enriched_question`
3. `get_text_embedding`
4. `get_followup_question`
5. `assess_followup_question`
6. `cosine_similarity`
7. `run_tests`

**test_web_pipeline.py**

This code sets up a series of unit tests using Pytest for validating web scraping and data processing functionalities. It utilizes logging to track the execution and ensure troubleshooting capability.

`create_mock_html_files` creates mock HTML files imitating real downloads for test purposes. 

The `test_chunk_addition` validates the file creation and chunk enrichment processes by ensuring specific content and structure in the generated JSON file.

Fixtures like `test_output_dir` and `config` help to set up and clean up the test environment, creating necessary instances and directories.

`check_content` verifies the presence of specific content in master JSON files.

`run_pipeline` runs a series of text enrichment processes.

`verify_hit_counts` ensures the hit counts are accurate.

Significant classes/functions: `create_mock_html_files`, `test_chunk_addition`, `check_content`, `run_pipeline`, `verify_hit_counts`, `test_web_pipeline`.

**test_youtube_pipeline.py**

This Python code is for testing a YouTube transcript processing pipeline using pytest. 

The `IgnoreSpecificWarningsFilter` class is used to filter out specific warning messages. 

The `test_output_dir` and `config` functions are pytest fixtures for setting up temporary directories and configuration objects.

`check_content` checks JSON files for specific content based on source IDs.

`run_pipeline` orchestrates a series of data enrichment processes on transcripts.

`verify_hit_counts` verifies the correctness of URL hit counts in the output data.

`test_youtube_pipeline` tests the entire pipeline, ensuring the downloading, processing, and verification steps work correctly.

`test_get_transcript_exceptions` and `test_get_transcript_success` test the `get_transcript` functions behavior under various scenarios, including exceptions and successful runs.
****************************************

****************************************
Boxer\scripts\text\ReadMe.Salon.md
****************************************
**enrich_lite.py**

- The script removes text and description fields from dictionaries in a JSON list and saves the modified list to a new JSON file.
- The `remove_text` function processes each dictionary in the list to exclude the "text" and "description" keys.
- The `enrich_lite` function logs any errors, loads the JSON file, processes its content using `remove_text`, and saves the modified data to a new JSON file.
- The `enrich_lite` function expects a `destinationDir` argument indicating where to find and save the files.
- Key elements include the `remove_text` function for filtering data and the `enrich_lite` function for orchestrating the file operations and logging.

**enrich_text_chunks.py**

This script converts markdown-based JSON transcript files into a master CSV file. 

The `MddSegment` class represents segments of the transcript, including text, start time, and duration. The `gen_metadata_master` function generates metadata for these segments, while `clean_text` removes unwanted characters from the text. The functions `append_text_to_previous_chunk` and `add_new_chunk` help transition between chunks smoothly and add new chunks if certain conditions are met. 

The `parse_json_mdd_transcript` function parses the JSON file, breaking down the transcript into smaller chunks while calculating token lengths and segment time duration. 

The `get_transcript` function processes each `.mdd` file to extract the transcript chunks. 

The `enrich_text_chunks` function coordinates the entire process and saves the extracted and processed chunks into a master JSON file. It uses the Tiktoken library to manage tokenization and the Rich library for progress tracking.

**enrich_text_embeddings.py**

1. This script creates text embeddings using the OpenAI API, specifically integrating with Azure's variant of OpenAI.
  
2. The `normalize_text` function cleans and normalizes text by removing unwanted spaces, newlines, and incorrect punctuation formats.

3. The `get_text_embedding` function retrieves embeddings for the provided text using the OpenAI API, with retry logic for error handling.

4. The `process_queue` function processes chunks of text by either fetching saved embeddings or generating new ones if necessary, then updates the output queue.

5. The `enrich_text_embeddings` function orchestrates the preparation and processing of text chunks, manages logging, configures threading, and ensures chunks are processed and saved correctly.

6. Key imports include `AzureOpenAI` for API calls, `tenacity` for retry logic, `rich.progress` for progress tracking, and various standard libraries for file handling and logging.

Classes/Functions:
- `normalize_text`
- `get_text_embedding`
- `process_queue`
- `enrich_text_embeddings`

**enrich_text_summaries.py**

This code is designed to process and summarize YouTube transcripts using ChatGPT provided by Azure OpenAI. The key functions and classes in the module are `Counter`, `chatgpt_summary`, `process_queue_for_summaries`, and `enrich_text_summaries`.

**Classes:**
- **Counter**: A thread-safe counter used to keep track of processed chunks.
    
**Functions:**
- **chatgpt_summary**: Generates a summary of input text using ChatGPT, with retry logic to handle rate limiting and transient errors.
- **process_queue_for_summaries**: Processes a queue containing text chunks, summarizing each chunk with ChatGPT and handling existing summaries.
- **enrich_text_summaries**: Configures the Azure OpenAI client, loads input data, and starts multiple threads to process text chunks in parallel. The summaries are saved in an output JSON file.

The code also includes necessary imports, setting up logging, and ensuring the existence of output directories.
****************************************

****************************************
Boxer\scripts\web\ReadMe.Salon.md
****************************************
**download_html.py**

This script downloads the text content of all subpages from a specified URL and saves it in JSON format.

Important classes and functions include:
- `Counter` class: A thread-safe counter.
- `makePathOnly`: Generates a clean path from a URL.
- `makeFullyQualified`: Forms a fully-qualified URL from a base and relative path.
- `get_html`: Downloads HTML content of a URL, processes it to extract text, and saves it in a JSON file.
- `process_queue`: Processes a queue of URLs, downloading HTML content in multiple threads.
- `deduplicate`: Removes duplicate links.
- `remove_exits`: Removes links pointing out of the main site.
- `add_prefix`: Adds prefixes to relative URLs.
- `recurse_page_list`: Recursively crawls pages starting from a URL.
- `build_page_list`: Builds a list of pages from a URL.
- `download_html`: Main function to manage the downloading process with multi-threading.
****************************************

****************************************
Boxer\scripts\youtube\ReadMe.Salon.md
****************************************
**download_transcripts.py**

The script downloads transcripts for videos in a YouTube playlist.

**Important Classes and Functions:**

- **Counter Class**: A thread-safe counter.
    - **increment()**: increments the counter value.
    
- **gen_metadata()**: Generates metadata for a video and saves it as a JSON file.
    - **Inputs**: playlist_item, transcriptDestinationDir.

- **get_transcript()**: Retrieves and saves the transcript of a video.
    - **Inputs**: playlist_item, counter_id, transcriptDestinationDir, logger.

- **process_queue()**: Processes videos in the queue for transcription and metadata generation.
    - **Inputs**: q, counter, transcriptDestinationDir, logger.

- **download_transcripts()**: Main function to initiate downloading transcripts.
    - **Inputs**: playlistId, transcriptDestinationDir.

The YouTube Data API fetches video details, and YouTubeTranscriptApi retrieves the transcripts. Transcripts and metadata are processed and saved in the specified directory. The process uses multithreading for efficiency.

**enrich_transcript_chunks.py**

This script generates a master csv file from transcript files by reading `.json` and `.vtt` files in a specified directory.

The important classes and functions include:

1. **VttChunk**: A class that initializes transcript chunks with start time, duration, and text.
2. **gen_metadata_master**: Generates metadata text for titles and descriptions.
3. **clean_text**: Cleans transcript text by removing unwanted characters and symbols.
4. **append_text_to_previous_chunk**: Appends a portion of the current text to the previous chunk to achieve context overlap.
5. **add_new_chunk**: Adds a new chunk to the list of transcript chunks.
6. **parse_json_vtt_transcript**: Parses the `.json.vtt` file and extracts transcript chunks.
7. **get_transcript**: Retrieves the transcript from the `.vtt` file.
8. **enrich_transcript_chunks**: Processes JSON and VTT transcript files, enriches the chunks, and saves the output.
9. **ensure_directory_exists**: Ensures that the directory for output files exists, creating it if necessary.

The script configures logging, uses `tiktoken` for encoding with the GPT-3.5-turbo model, and utilizes `rich` for progress tracking.

**enrich_transcript_embeddings.py**

This code leverages the OpenAI API for text embedding and log handling within a multi-threaded environment. It starts by importing necessary libraries and modules, including logging, regex, JSON, threading, and queue.

Key functions include:

- `normalize_text`: Cleans text by removing extra spaces and newlines.
- `get_text_embedding`: Retries API requests until successful or after multiple attempts.
- `process_queue`: Processes chunks in a queue, retrieving or generating embeddings, handling errors, and updating progress.
- `convert_time_to_seconds`: Converts timestamps to seconds.
- `enrich_transcript_embeddings`: Main function initializing the API client, setting up logging, reading input, and processing text chunks using multiple threads.

Overall, it reads transcript data, processes it to generate or retrieve embeddings from an OpenAI API, and writes the enriched output back to a file.

**enrich_transcript_summaries.py**

1. **Imports:** The code imports various libraries including standard library modules (`json`, `os`, `threading`, `queue`, `logging`), third-party packages (`openai`, `tenacity`, `rich.progress`), and local modules (`common.common_functions`, `common.ApiConfiguration`).

2. **Class: `Counter`:** A thread-safe counter with a lock mechanism to handle concurrent increments.

3. **Function: `chatgpt_summary`:** A retry-decorated function that uses the AzureOpenAI's `chat` API to generate summaries, retrying on certain errors up to 5 times.

4. **Function: `process_queue`:** Processes text chunks from a queue, generating summaries using `chatgpt_summary`, and handling errors. Updates progress and stores results in a shared list.

5. **Function: `convert_time_to_seconds`:** Converts a `HH:MM:SS` formatted string to seconds.

6. **Function: `enrich_transcript_summaries`:** Loads transcript chunks, processes them using multiple threads, and combines summaries with existing data. Saves the results to a file after processing.

This code is designed to parallelize obtaining summaries for blocks of text that likely come from video transcripts, enrich existing data, and save the results efficiently.

**not_used_enrich_transcript_speaker.py**

This script extracts speaker names from YouTube video metadata and the first minute of the transcript using OpenAI Functions entity extraction.

Important classes and functions:
1. `Counter` class: Provides a thread-safe counter for tracking processed items.
2. `get_speaker_info` function: Uses OpenAI API to extract speaker names from text.
3. `clean_text` function: Cleans the input text by removing unwanted characters.
4. `get_first_segment` function: Reads the first segment of the video's transcript.
5. `process_queue` function: Processes files in a queue, extracting and saving speaker names.

The script uses threading to improve efficiency, retries API requests on failure, and logs progress. It reads metadata and transcripts from JSON files, calls the OpenAI API for entity extraction, and updates files with the extracted speaker names.
****************************************

****************************************
Teams\src\functions\ReadMe.Salon.md
****************************************
**boxer.ts**

**Important Functions:**
- `boxer`

**Summary:**
- The code provides a starter kit for implementing server-side logic for a Teams App using TypeScript and Azure Functions, with reference to Azure Functions documentation for a complete developer guide.
  
- It defines the `boxer` function that handles HTTP requests and extracts the "question" parameter from the request query or body.
  
- It utilizes the `axios` library with retry capabilities for making HTTP requests to the external Braid API.

- The function queries an external API with the extracted question and processes the API response by mapping the results into a formatted array of items.

- It returns an HTTP response containing the processed boxer information or an error status if the request is invalid or an internal error occurs.

- The `app.http` method registers the `boxer` function to handle HTTP GET requests without requiring authentication.
****************************************
